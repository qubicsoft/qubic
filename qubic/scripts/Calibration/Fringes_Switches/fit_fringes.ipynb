{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9685f407",
   "metadata": {},
   "source": [
    "## Fit the fringe measurements and intercalibrate the TES\n",
    "\n",
    "Edited by Louise 16/03/2021\n",
    "\n",
    "In this notebook, I use an analytical model to simulate the fringes, defined in the library `selfcal_lib.py`. The goal is to fit the fringes measurements and measure the focal length of the combiner f, the off-axis angle of the source theta and the gain of each detectors. This is a good manner to intercalibrate the TES. \n",
    "\n",
    "The measurements are .fits files generated by the notebook `scripts/Calibration/Fringes_Switches/Generate-Fringes-Oct-2020-Louise.Rmd` that makes the fringes analysis from the raw TODs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ebaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import SymLogNorm\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import scipy.optimize as sop\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import pandas as pd\n",
    "import emcee\n",
    "import corner\n",
    "from iminuit import Minuit\n",
    "\n",
    "import qubic\n",
    "from qubic import selfcal_lib as scal\n",
    "from qubicpack.utilities import Qubic_DataDir\n",
    "from qubic import fringes_lib as flib\n",
    "import qubic.fibtools as ft\n",
    "\n",
    "rc('figure', figsize=(12, 6))\n",
    "rc('font', size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab18a4",
   "metadata": {},
   "source": [
    "### Choose if you work with real data or with simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c5af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fe231",
   "metadata": {},
   "source": [
    "### Get the measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9fbfce",
   "metadata": {},
   "source": [
    "Get the .fits file with the fringes measurement. You must put your personal directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dir = '/home/lmousset/QUBIC/Qubic_work/Calibration/datas/Fringes/'\n",
    "myfringes = 'Fringes_2020-10-27_12BLs_RemoveSlopePerTES_medianTrue_refTESautomatic_maskbadTES0.75.fits'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d3748e",
   "metadata": {},
   "source": [
    "Read informations saved in the fits file and make a QubicInstrument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f17807",
   "metadata": {},
   "outputs": [],
   "source": [
    "header, fdict = flib.read_fits_fringes(global_dir + myfringes)\n",
    "print(fdict.keys())\n",
    "\n",
    "allfringes = fdict['FRINGES_1D']\n",
    "allerr = fdict['ERRORS']\n",
    "\n",
    "# Normalization \n",
    "std = np.std(allfringes)\n",
    "for k in range(len(allfringes)):\n",
    "#     std = np.nanstd(allfringes[k])\n",
    "    allfringes[k] /= std\n",
    "    allerr[k] /= std\n",
    "\n",
    "allmask_bad_TES = fdict['MASK_BAD_TES']\n",
    "BLs = fdict['BLS']\n",
    "nimages = len(BLs)\n",
    "\n",
    "xTES = fdict['X_TES']\n",
    "yTES = fdict['Y_TES']\n",
    "# print(BLs)\n",
    "\n",
    "# Make a QUBIC instrument\n",
    "d = qubic.qubicdict.qubicDict()\n",
    "d.read_from_file('global_source_oneDet.dict')\n",
    "d['nf_sub'] = 1\n",
    "d['Multiband'] = False\n",
    "q = qubic.QubicInstrument(d)\n",
    "\n",
    "\n",
    "BLs_sort, BLs_type = scal.find_equivalent_baselines(BLs, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cc76f",
   "metadata": {},
   "source": [
    "Plot the baselines. There are sorted by equivalence type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea237ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scal.plot_BLs_eq(BLs, BLs_sort, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1dc55a",
   "metadata": {},
   "source": [
    "### Detect bad detectors\n",
    "\n",
    "This is also done at theend of the Notebook `scripts/Calibration/Fringes_Switches/Generate-Fringes-Oct-2020-Louise.Rmd`. To be set as bad, the TES must be NAN in at least N images. This N can be chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d40b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a loop on N, compute how many bad TES it gives to choose the N you want.\n",
    "thecond = np.arange(2, 12)\n",
    "nbad = []\n",
    "\n",
    "for cond in thecond:\n",
    "    the_mask = flib.decide_bad_TES(allmask_bad_TES, condition=cond)\n",
    "#     print(the_mask)\n",
    "    nbad.append(int(256 - np.nansum(the_mask)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thecond, nbad, 'bo')\n",
    "plt.xlabel('Number of images where the TES is NAN')\n",
    "plt.ylabel('Number of bad TES')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be7869",
   "metadata": {},
   "source": [
    "There is a plateau around N=9,10 which leads to ~30 bad detectors. We will choose N=30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef24c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "the_mask = flib.decide_bad_TES(allmask_bad_TES, condition=9)\n",
    "nbad = int(256 - np.nansum(the_mask))\n",
    "print(nbad)\n",
    "\n",
    "# print(the_mask)\n",
    "flib.plot_fringes_scatter(q, xTES, yTES, the_mask, normalize=False, s=140, cbar=False)\n",
    "\n",
    "badTES = flib.give_index_bad_TES(the_mask)\n",
    "print(badTES.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb7e0b",
   "metadata": {},
   "source": [
    "### Plot the fringes measurements. \n",
    "We make 2 plots:\n",
    "* A simple scatter plot\n",
    "* A imshow plot after performing a Gaussian convolution using Astropy (just for visual help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color map whith bad detectors in black\n",
    "cmap_bwr = flib.make_cmap_nan_black('bwr')\n",
    "\n",
    "for k in range(nimages):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 7))\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    fig.suptitle(f'k={k} - BL {BLs[k]}')\n",
    "    ax0, ax1 = axs.ravel()\n",
    "    \n",
    "    # Scatter plot\n",
    "    flib.plot_fringes_scatter(q, xTES, yTES, allfringes[k] * the_mask, normalize=True, s=180, fig=fig, ax=ax0)\n",
    "\n",
    "\n",
    "    # Plot with Astropy convolution \n",
    "    fringes2D = flib.make2Dfringes_data(allfringes[k] * the_mask)\n",
    "    fringes2D_conv = flib.astropy_convolution(fringes2D, sigma=0.7)\n",
    "    flib.plot_fringes_imshow(fringes2D_conv, normalize=True, fig=fig, ax=ax1, cmap=cmap_bwr, \n",
    "                             title='Gaussian convolution', mask=flib.make_mask2D_thermometers_TD())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for papers\n",
    "# k = 5\n",
    "# fig = plt.figure(figsize=(7, 7))\n",
    "# ax = fig.gca()\n",
    "# flib.plot_fringes_scatter(q, xTES, yTES, allfringes[k] * the_mask, normalize=False, s=350,\n",
    "#                           fig=fig, ax=ax, vmin=-0.3, vmax=0.3, title=f'Data - Baseline {BLs[k]}', fontsize=20)\n",
    "# fig.tight_layout()\n",
    "# fig.savefig('/home/lmousset/QUBIC/Images/Data20201027_fringes_49-51.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f8347",
   "metadata": {},
   "source": [
    "### Remove thermometers \n",
    "\n",
    "Data contains 256 detectors, 248 are bolometers but 8 are thermometers. To compare with Qubic soft simulations, it is useful to remove thermometers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a8921",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata, ydata, the_mask = flib.remove_thermometers(xTES, yTES, the_mask)\n",
    "\n",
    "ndet = xdata.shape[0]\n",
    "print('Number of detectors:', ndet)\n",
    "\n",
    "data, error = [], []\n",
    "for k in range(nimages):\n",
    "    _, _, mydata = flib.remove_thermometers(xTES, yTES, allfringes[k])\n",
    "    _, _, myerror = flib.remove_thermometers(xTES, yTES, allerr[k])\n",
    "    data.append(mydata)\n",
    "    error.append(myerror)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef69d10",
   "metadata": {},
   "source": [
    "### Re-order data as simulations from Qubic soft\n",
    "TES numbering on the instrument and in simulations are different. To compare the two, we re-order the data following simulation order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xONAFP, yONAFP, _ = scal.get_TEScoordinates_ONAFP(q)\n",
    "the_mask = flib.reorder_data(the_mask, xdata, ydata, xONAFP, yONAFP)\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# the_mask[246] = np.nan\n",
    "# the_mask[230] = np.nan\n",
    "\n",
    "fringes, errs = [], []\n",
    "for k in range(nimages):\n",
    "    fringes.append(flib.reorder_data(data[k], xdata, ydata, xONAFP, yONAFP))\n",
    "    errs.append(flib.reorder_data(error[k], xdata, ydata, xONAFP, yONAFP))\n",
    "\n",
    "\n",
    "# Check the re-ordering is correct, the 2 plots for each baseline should be identical.\n",
    "vmin = -1\n",
    "vmax = 1\n",
    "for k in range(nimages):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f'BL {BLs[k]}')\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    ax0, ax1 = axs\n",
    "    flib.plot_fringes_scatter(q, xdata, ydata, data[k]*the_mask, \n",
    "                              normalize=True, vmin=vmin, vmax=vmax,\n",
    "                              s=180, fig=fig, ax=ax0,\n",
    "                              title='Original order')\n",
    "    \n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, fringes[k]*the_mask, \n",
    "                              normalize=True, vmin=vmin, vmax=vmax,\n",
    "                              s=180, fig=fig, ax=ax1,\n",
    "                              title='Re-ordered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade8b273",
   "metadata": {},
   "source": [
    "### Make a selection\n",
    "If you want to perform the fit on a reduce number of images, you can select them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d29f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = True\n",
    "if selection:\n",
    "    myselection = [2, 5, 11]\n",
    "#     myselection = [1, 2, 4, 5, 7, 8, 9, 10, 11]\n",
    "#     myselection = [0, 1, 2, 4, 5, 11]\n",
    "    remind_all_fringes = fringes.copy() \n",
    "    fringes = [fringes[i] for i in myselection]\n",
    "    errs = [errs[i] for i in myselection]\n",
    "    BLs = [BLs[i] for i in myselection]\n",
    "    print('Selected baselines:', BLs)\n",
    "    \n",
    "nimages = len(BLs)\n",
    "print(f'We will work with {nimages} images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c12a2f",
   "metadata": {},
   "source": [
    "# Start fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ef5d4e",
   "metadata": {},
   "source": [
    "#### Make fake data\n",
    "\n",
    "This is only useful if you work with a simulation and not with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndet = 248 \n",
    "print('ndet:', ndet)\n",
    "\n",
    "# Parameters for the fit\n",
    "focal_fake = 0.29\n",
    "theta_fake = np.deg2rad(0.5)\n",
    "allP_fake = [0.5] * nimages\n",
    "# allP_fake = list(np.random.rand(nimages))\n",
    "print('P_k fake:', allP_fake)\n",
    "params_fake = [focal_fake, theta_fake] + allP_fake\n",
    "\n",
    "\n",
    "# Gain for each TES (same for each image)\n",
    "# gains_fake = np.ones(ndet)\n",
    "gains_fake = np.random.normal(1., 1., size=ndet)\n",
    "gains_fake /= np.mean(gains_fake)\n",
    "print('gain mean:', np.mean(gains_fake))\n",
    "print('gains fake:', gains_fake[:10])\n",
    "print('gains negative:', gains_fake[gains_fake<0.])\n",
    "\n",
    "sigma = 0.03 # Gaussian noise\n",
    "\n",
    "fake_fringes = []\n",
    "allPhi_fake = []\n",
    "d['focal_length'] = focal_fake\n",
    "q = qubic.QubicInstrument(d)\n",
    "for k in range(nimages):\n",
    "    model_fake_data = scal.Model_Fringes_Ana(q, BLs[k], \n",
    "                                             theta_source=theta_fake, \n",
    "                                             nu_source=150e9,\n",
    "                                             frame='ONAFP')\n",
    "\n",
    "    x, y, Phi = model_fake_data.get_fringes(times_gaussian=False)\n",
    "    allPhi_fake.append(Phi)\n",
    "    \n",
    "    # Multiply by a global amplitude (Calibration source power)\n",
    "    fake_P = Phi * allP_fake[k]\n",
    "    \n",
    "    # Gain\n",
    "    fake_gain = fake_P * gains_fake\n",
    "    \n",
    "    # Add gaussian noise\n",
    "    noise = np.random.normal(loc=0., scale=sigma, size=ndet)\n",
    "    print('Gaussian noise:', noise[:10])\n",
    "    fake_noise = fake_gain + noise\n",
    "    \n",
    "    fake_fringes.append(fake_noise)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    fig.subplots_adjust(wspace=0.5)\n",
    "    ax0, ax1, ax2, ax3 = np.ravel(axs)\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, Phi, frame='ONAFP', \n",
    "                         fig=fig, ax=ax0, title='Pure fringes', unit=None, s=170, cmap='bwr')\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, fake_P, frame='ONAFP', \n",
    "                         fig=fig, ax=ax1, title='Fringes x Power', unit=None, s=170, cmap='bwr')\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, fake_gain, frame='ONAFP', \n",
    "                         fig=fig, ax=ax2, title='With Gains', unit=None, s=170, cmap='bwr')\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, fake_noise, frame='ONAFP',\n",
    "                         fig=fig, ax=ax3, title='Adding noise', unit=None, s=170, cmap='bwr')\n",
    "\n",
    "if simu:\n",
    "    fringes = fake_fringes\n",
    "    errs = list(np.ones_like(fake_fringes) * sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043bf11",
   "metadata": {},
   "source": [
    "#### Covariance matrix of the noise\n",
    "\n",
    "To eliminate bad TES, we put a very high error on them => very small weight in the fit.\n",
    "\n",
    "Then we make a list with an inverse covariance matrix for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74975a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allInvCov = []\n",
    "for k in range(nimages):\n",
    "    errs[k][np.isnan(the_mask)] *= 1e20\n",
    "\n",
    "    allInvCov.append(scal.make_inverse_covariance(errs[k], verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e0319",
   "metadata": {},
   "source": [
    "#### Explore the chi2 to find guess parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f64e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fl, all_th, chi2_grid = scal.make_chi2_grid(allInvCov, fringes, BLs, q, \n",
    "                                nval_fl=40, nval_th=40, \n",
    "                                fl_min=0.25, fl_max=0.35,\n",
    "                                th_min=np.deg2rad(-1), th_max=np.deg2rad(1), \n",
    "                                LogPower=-1,\n",
    "                                fixPower=True)\n",
    "\n",
    "# all_fl, all_th, chi2_grid, popt = scal.make_chi2_grid(allInvCov, fringes, BLs, q, \n",
    "#                                 nval_fl=10, nval_th=10, \n",
    "#                                 fl_min=0.25, fl_max=0.35,\n",
    "#                                 th_min=np.deg2rad(-1), th_max=np.deg2rad(1), \n",
    "#                                 LogPower=-1,\n",
    "#                                 fixPower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175333e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth with a gaussian to avoid loosing the min in random pixel very low (not always necessary).\n",
    "smooth = False\n",
    "step_fl = all_fl[1] - all_fl[0]\n",
    "step_th = all_th[1] - all_th[0]\n",
    "if smooth:\n",
    "    chi2_grid = gaussian_filter(chi2_grid, sigma=[step_fl*4e2, step_th*4e2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the min and take it as a guess for the following\n",
    "min_indices = np.unravel_index(np.argmin(chi2_grid), chi2_grid.shape)\n",
    "print(f'Chi2 min = {np.min(chi2_grid)} at {min_indices}')\n",
    "\n",
    "fl_guess = all_fl[min_indices[0]]\n",
    "th_guess = all_th[min_indices[1]]\n",
    "\n",
    "allP_guess = [0.5] * nimages\n",
    "\n",
    "\n",
    "params_guess = [fl_guess, th_guess] + allP_guess\n",
    "\n",
    "print('Guess:', params_guess)\n",
    "if simu:\n",
    "    print('Fake:', params_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6375d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the chi2 map \n",
    "def plot_chi2_map(all_th, all_fl, chi2_grid,\n",
    "                  vmin=0, vmax=1e10, norm=None, \n",
    "                  thetas=[], focals=[], labels=[]):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    c = ax.pcolor(np.rad2deg(all_th), all_fl, chi2_grid, vmin=0, vmax=5e8, shading='auto')#, norm=SymLogNorm(99e9))\n",
    "    for i in range(len(thetas)):\n",
    "        ax.scatter(np.rad2deg(thetas[i]), focals[i],  marker='o', s=100, label=labels[i])\n",
    "    ax.set_xlabel('Theta')\n",
    "    ax.set_ylabel('Focal length')\n",
    "    fig.colorbar(c, ax=ax)\n",
    "    ax.legend()\n",
    "    \n",
    "    return\n",
    "\n",
    "if simu:\n",
    "    plot_chi2_map(all_th, all_fl, chi2_grid,\n",
    "                  vmin=0, vmax=1e10, norm=None,\n",
    "                  thetas=[th_guess, theta_fake], \n",
    "                  focals=[fl_guess, focal_fake], \n",
    "                  labels=['Guess', 'fake'])\n",
    "else:\n",
    "    plot_chi2_map(all_th, all_fl, chi2_grid,\n",
    "                  vmin=0, vmax=1e10, norm=None,\n",
    "                  thetas=[th_guess], \n",
    "                  focals=[fl_guess], \n",
    "                  labels=['Guess'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877572f2",
   "metadata": {},
   "source": [
    "## Minimize the chi2 \n",
    "\n",
    "Using `scipy.optimize.minimize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f0eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_guess = [0.30, np.deg2rad(-0.5)] + [0.5]*3\n",
    "# result = sop.minimize(scal.get_chi2, \n",
    "#                       x0=params_guess, \n",
    "#                       args=(allInvCov, fringes, BLs, q), \n",
    "#                       method='Nelder-Mead',\n",
    "#                       options={'maxiter':10000})\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e4384",
   "metadata": {},
   "source": [
    "#### Using iMinuit\n",
    "\n",
    "We can change:\n",
    "   * the guess\n",
    "   * fix: fix parameters to a given value\n",
    "   * error: the step it moves around the parameter during minimization\n",
    "   * limit: bounds for the parameters (ex: focal and Pk positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20505656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_minuit(params):\n",
    "    chi2 = scal.get_chi2(params, allInvCov, fringes, BLs, q, nu_source=150e9, returnA=False)\n",
    "    return chi2\n",
    "\n",
    "# Choose a guess by hand\n",
    "params_guess = [0.3, np.deg2rad(0.5)] + [-1.5] * nimages\n",
    "\n",
    "# Initialize iMinuit\n",
    "m = Minuit.from_array_func(chi2_minuit, \n",
    "                           params_guess, \n",
    "                           errordef=1, print_level=2,\n",
    "                           error=[2e-3, np.deg2rad(0.01)] + [0.001] * nimages,\n",
    "                           fix=[False, False] + [False]*(nimages-1) + [False],\n",
    "                           limit=[(None, None), (None, None)] + [(-15, 15)] * nimages)\n",
    "# Run the minimization\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e774c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters\n",
    "chi2_Minuit = m.fcn(m.np_values())\n",
    "\n",
    "fl_minuit = m.np_values()[0]\n",
    "th_minuit = m.np_values()[1]\n",
    "LogPk_minuit = m.np_values()[2:]\n",
    "Pk_minuit = 10**(LogPk_minuit)\n",
    "\n",
    "print('***** Focal:')\n",
    "if simu:\n",
    "    print('Fake:', focal_fake)\n",
    "print('Result:', fl_minuit)\n",
    "print('Guess:', params_guess[0])\n",
    "\n",
    "print('\\n***** Theta:')\n",
    "if simu:\n",
    "    print('Fake:', np.rad2deg(theta_fake))\n",
    "print('Result:', np.round(np.rad2deg(th_minuit), 6))\n",
    "print('Guess:', np.round(np.rad2deg(params_guess[1]), 6))\n",
    "\n",
    "print('\\n***** Power Pk:')\n",
    "print('Guess:', [10**a for a in params_guess[2:]])\n",
    "print('Result:', Pk_minuit)\n",
    "if simu:\n",
    "    print('Fake:', np.round(allP_fake, 4))\n",
    "    print('Fake / Result:', np.round(allP_fake / Pk_minuit, 4))\n",
    "\n",
    "NDDL = nimages * ndet - (ndet + nimages + 2)\n",
    "print('\\nReduce Chi2:', chi2_Minuit / NDDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11fbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scal.get_chi2([0.3, np.deg2rad(0.5), 1.6e-4, 2.6e-2], allInvCov, fringes, BLs, q, nu_source=150e9, returnA=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_guess = params_guess[0]\n",
    "th_guess = params_guess[1]\n",
    "if simu:\n",
    "    plot_chi2_map(all_th, all_fl, chi2_grid,\n",
    "                  vmin=0, vmax=1e10, norm=None,\n",
    "                  thetas=[th_guess, theta_fake, th_minuit], \n",
    "                  focals=[fl_guess, focal_fake, fl_minuit], \n",
    "                  labels=['Guess', 'Fake', 'Minuit'])\n",
    "else:\n",
    "    plot_chi2_map(all_th, all_fl, chi2_grid,\n",
    "                  vmin=0, vmax=1e10, norm=None,\n",
    "                  thetas=[th_guess, th_minuit], \n",
    "                  focals=[fl_guess, fl_minuit], \n",
    "                  labels=['Guess', 'Minuit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaab134",
   "metadata": {},
   "source": [
    "#### Get the intercalibrations\n",
    "We compute the detector gains using the result of the minimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a958c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using minimize\n",
    "# th_source = result['x'][1]\n",
    "# q.optics.focal_length = result['x'][0]\n",
    "# allP_res = result['x'][2:]\n",
    "\n",
    "#Using Minuit\n",
    "th_source = th_minuit\n",
    "q.optics.focal_length = fl_minuit\n",
    "\n",
    "PowerPhi = []\n",
    "for k in range(nimages):\n",
    "    model = scal.Model_Fringes_Ana(q, BLs[k], \n",
    "                                   theta_source=th_source, \n",
    "                                   nu_source=150e9, \n",
    "                                   frame='ONAFP')\n",
    "\n",
    "    x, y, Phi = model.get_fringes(times_gaussian=False)\n",
    "    \n",
    "    # Global amplitude\n",
    "    PowerPhi.append(Phi * Pk_minuit[k])\n",
    "    \n",
    "\n",
    "# Gain for each detector\n",
    "A, Cov_A = scal.get_gains(PowerPhi, allInvCov, fringes)\n",
    "\n",
    "print('Gains found:\\n', np.round(A[:10], 4))\n",
    "print('Diagonal Cov_A:\\n', np.diag(Cov_A[:10]))\n",
    "if simu:\n",
    "    print('\\nGains fake:\\n', np.round(gains_fake[:10], 4))\n",
    "    \n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(x=np.arange(ndet), y=A, yerr=np.sqrt(np.diag(Cov_A)),\n",
    "            fmt='o')\n",
    "plt.errorbar(x=np.arange(ndet)[np.isnan(the_mask)], y=A[np.isnan(the_mask)], yerr=np.sqrt(np.diag(Cov_A))[np.isnan(the_mask)],\n",
    "            fmt='o', color='k')\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel('Detector index')\n",
    "plt.ylabel('Gains')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc288f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average A, it should be 1\n",
    "weights = 1/np.diag(Cov_A)\n",
    "avgA = np.average(A, weights=weights)\n",
    "print(avgA)\n",
    "\n",
    "print('Max A', np.nanmax(A*the_mask))\n",
    "print('Min A', np.nanmin(A*the_mask))\n",
    "\n",
    "# print(np.sort(A*the_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simu:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    ax1, ax2, ax3, ax4 = np.ravel(axs)\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, gains_fake, fig=fig, ax=ax1, frame='ONAFP', title='Gains fake', \n",
    "                         unit=None, vmin=None, vmax=None, s=150, cmap='bwr')\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, A, fig=fig, ax=ax2, frame='ONAFP', title='Gains found', \n",
    "                         unit=None, vmin=None, vmax=None, s=150, cmap='bwr')\n",
    "    scal.scatter_plot_FP(q, xONAFP, yONAFP, A-gains_fake, fig=fig, ax=ax3, frame='ONAFP', title='Residuals', \n",
    "                         unit=None, vmin=None, vmax=None, s=150, cmap='bwr')\n",
    "    mean = np.mean(A-gains_fake)\n",
    "    std = np.std(A-gains_fake)\n",
    "    ax4.hist(A-gains_fake, range=(-1, 1), bins=30, label='{:.6f} +- {:.6f}'.format(mean, std))\n",
    "    ax4.axvline(mean, color='r')\n",
    "    ax4.set_title('Histogram residuals')\n",
    "    ax4.legend()\n",
    "    fig.tight_layout()\n",
    "else:\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(13, 4))\n",
    "    ax1, ax2 = np.ravel(axs)\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, A*the_mask, \n",
    "                              fig=fig, ax=ax1, title='Gains found', \n",
    "                              vmin=-10, vmax=10, normalize=False,\n",
    "                              s=100)\n",
    "    \n",
    "    ax2.hist(A*the_mask, bins=30, range=(-100, 100))\n",
    "    ax2.set_xlabel('Gains found')\n",
    "    ax2.axvline(avgA, color='r', label=f'Mean = {avgA:.3f}')\n",
    "    ax2.legend()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simu:\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    ax1, ax2 = np.ravel(axs)\n",
    "\n",
    "    ax1.plot(allP_fake, result['x'][2:], 'ro')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='y=x')\n",
    "    ax1.set_xlabel('P Fake Data')\n",
    "    ax1.set_ylabel('P Fit result')\n",
    "    ax1.set_title('Power')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.errorbar(gains_fake, A, yerr=np.sqrt(np.diag(Cov_A)), fmt='o', color='b')\n",
    "    # ax2.plot(gains_fake, A, 'b.')\n",
    "    ax2.plot(gains_fake, gains_fake, 'k--', label='y=x')\n",
    "#     ax2.set_ylim(-5, 5)\n",
    "    ax2.set_xlabel('Gain Fake Data')\n",
    "    ax2.set_ylabel('Gain Fit result')\n",
    "    ax2.set_title('Gain')\n",
    "    ax2.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bf34b",
   "metadata": {},
   "source": [
    "#### Residuals and correction by intercalibrations (gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_colorbar(ax, image):\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    clb = fig.colorbar(image, cax=cax)\n",
    "    return\n",
    "\n",
    "def get_pull(data, errors, gains, PowerPhi, mask=None):\n",
    "    if mask is not None:\n",
    "        data *= mask\n",
    "    correct = data/gains\n",
    "    residu = correct - PowerPhi\n",
    "    pull = residu / (errors/gains)\n",
    "    \n",
    "    return correct, residu, pull\n",
    "    \n",
    "def plot_residuals(q, xONAFP, yONAFP, BL, data, the_mask, PowerPhi, errors, gains, cmap='bwr', \n",
    "                   normalize=True, vmin=-1, vmax=1, s=120):\n",
    "    \"\"\"Make all plots, data, correction, residuals, model...\n",
    "    1D plots can only work with diagonal fringes.\"\"\"\n",
    "    \n",
    "    # Orientation of the baseline\n",
    "    theta, _, _ = scal.give_bs_pars(q, BL)\n",
    "    if int(theta)== -45:\n",
    "        anti_diag = True\n",
    "    else:\n",
    "        anti_diag = False\n",
    "    print(int(theta))\n",
    "    \n",
    "    # Compute residuals, pull and mask the data\n",
    "    correct, residu, pull = get_pull(data, errors, gains, PowerPhi, mask=the_mask)\n",
    "    \n",
    "    fig, axs = plt.subplots(5, 2, figsize=(12, 20))\n",
    "    fig.suptitle(f'BL {BL}')\n",
    "    axs = np.ravel(axs)\n",
    "    \n",
    "    # Initial / corrected \n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, data, \n",
    "                              fig=fig, ax=axs[0], cmap=cmap, title='Data', s=s, \n",
    "                              normalize=normalize, vmin=vmin, vmax=vmax)\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, correct, \n",
    "                              fig=fig, ax=axs[1], cmap=cmap, title='Data / gains', s=s, \n",
    "                              normalize=normalize, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    # Plot fringes and corrected fringes in 1D diagonal\n",
    "    data2D = flib.make2Dfringes_QubicSoft(data, q, nan2zero=True)\n",
    "    flib.plot_fringes_diagonal(data2D, idiag=[-3, -4, -2, -1, 0, 1, 2, 3, 4], anti_diag=anti_diag,\n",
    "                          fig=fig, ax=axs[2], ylim=(-0.2, 0.2), title='Data')\n",
    "    \n",
    "    correct2D = flib.make2Dfringes_QubicSoft(correct, q, nan2zero=True)\n",
    "    flib.plot_fringes_diagonal(correct2D, idiag=[-3, -4, -2, -1, 0, 1, 2, 3, 4], anti_diag=anti_diag, \n",
    "                          fig=fig, ax=axs[3], ylim=(-0.2, 0.2), title='Data / gains')\n",
    "        \n",
    "    # Fit\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, PowerPhi,\n",
    "                              fig=fig, ax=axs[4], cmap=cmap, title='Fit: Power x Phi', s=s,\n",
    "                              normalize=False, vmin=-0.01, vmax=0.01)\n",
    "    \n",
    "    # Residuals\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, residu,\n",
    "                             fig=fig, ax=axs[5], cmap=cmap, title='Residuals = Data/gains - Power x Phi', s=s,\n",
    "                             normalize=False, vmin=-0.1, vmax=0.1)\n",
    "    \n",
    "    # Plot model and residuals 1D diagonal\n",
    "    PowerPhi2D = flib.make2Dfringes_QubicSoft(PowerPhi, q, nan2zero=True)\n",
    "    flib.plot_fringes_diagonal(PowerPhi2D, idiag=[-3, -4, -2, -1, 0, 1, 2, 3, 4], anti_diag=anti_diag,\n",
    "                          fig=fig, ax=axs[6], title='Fit')\n",
    "    \n",
    "    residu2D = flib.make2Dfringes_QubicSoft(residu, q, nan2zero=True)\n",
    "    flib.plot_fringes_diagonal(residu2D, idiag=[-3, -4, -2, -1, 0, 1, 2, 3, 4], anti_diag=anti_diag,\n",
    "                          fig=fig, ax=axs[7], ylim=(-0.2, 0.2), title='Residuals')\n",
    "  \n",
    "    # Pull\n",
    "    mean_pull = np.nanmean(pull)\n",
    "    std_pull = np.nanstd(pull)\n",
    "    axs[8].hist(pull, range=(-15, 15), bins=15, label='{:.5f} +- {:.5f}'.format(mean_pull, std_pull))\n",
    "    axs[8].axvline(mean_pull, color='r')\n",
    "    axs[8].axvline(mean_pull+std_pull, color='r', linestyle='--')\n",
    "    axs[8].axvline(mean_pull-std_pull, color='r', linestyle='--')\n",
    "    axs[8].set_title('Pull = Residuals / (errors/gains)')\n",
    "    axs[8].legend()\n",
    "    \n",
    "    # Histogram Data/Gain and Residuals\n",
    "    axs[9].hist(correct*the_mask, bins=20, alpha=0.2, label='Data/Gain', range=(-0.3, 0.3))\n",
    "    axs[9].hist(residu*the_mask, bins=20, alpha=0.2, label='Residuals', range=(-0.3, 0.3))\n",
    "    axs[9].legend()\n",
    "    axs[9].set_title(f'STD(data/gain) / STD(Residus) = {np.nanstd(correct/residu):.3f}')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct, residu, pull = get_pull(fringes[0], errs[0], A, PowerPhi[0])\n",
    "# pull*the_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_bwr=flib.make_cmap_nan_black('bwr')\n",
    "allmean_pull, allstd_pull = [], []\n",
    "for k in range(nimages):\n",
    "    \n",
    "    plot_residuals(q, xONAFP, yONAFP, BLs[k], fringes[k], the_mask, PowerPhi[k], errs[k], A, \n",
    "                   cmap=cmap_bwr, normalize=True, vmin=-1, vmax=1, s=100)\n",
    "    _, _, pull = get_pull(fringes[k], errs[k], A, PowerPhi[k])\n",
    "    allmean_pull.append(np.mean(pull))\n",
    "    allstd_pull.append(np.std(pull))\n",
    "print(allstd_pull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b8c8b",
   "metadata": {},
   "source": [
    "### Correct the errors to have std(Pull) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb70db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = np.any([np.abs(i - 1) > 0.5 for i in allstd_pull])\n",
    "condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828c60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while condition:\n",
    "    i += 1\n",
    "    print(f'\\n***** Run {i} ************* STD Pull:', allstd_pull)\n",
    "    # Correction a posteriori (bidouille to have STD(Pull) = 1)\n",
    "    allInvCov = []\n",
    "    for k in range(nimages):\n",
    "        errs[k] *= allstd_pull[k]\n",
    "        allInvCov.append(scal.make_inverse_covariance(errs[k], verbose=False))\n",
    "\n",
    "    # Minimization\n",
    "    m = Minuit.from_array_func(chi2_minuit, \n",
    "                               params_guess, \n",
    "                               errordef=1, print_level=2,\n",
    "                               error=[2e-3, np.deg2rad(0.01)] + [0.001] * nimages,\n",
    "                               fix=[False]*(nimages+2),\n",
    "                               limit=[(None, None), (None, None)] + [(None, None)] * nimages)\n",
    "    m.migrad()\n",
    "    fl_minuit = m.np_values()[0]\n",
    "    th_minuit = m.np_values()[1]\n",
    "    Pk_minuit = 10**(m.np_values()[2:])\n",
    "\n",
    "    #Using Minuit\n",
    "    th_source = th_minuit\n",
    "    q.optics.focal_length = fl_minuit\n",
    "\n",
    "    PowerPhi = []\n",
    "    for k in range(nimages):\n",
    "        model = scal.Model_Fringes_Ana(q, BLs[k], \n",
    "                                       theta_source=th_source, \n",
    "                                       nu_source=150e9, \n",
    "                                       frame='ONAFP')\n",
    "\n",
    "        x, y, Phi = model.get_fringes(times_gaussian=False)\n",
    "\n",
    "        # Global amplitude\n",
    "        PowerPhi.append(Phi * Pk_minuit[k])\n",
    "\n",
    "\n",
    "    # Gain for each detector\n",
    "    A, Cov_A = scal.get_gains(PowerPhi, allInvCov, fringes)\n",
    "\n",
    "    print('Gains found:\\n', np.round(A[:10], 4))\n",
    "    print('Diagonal Cov_A:\\n', np.diag(Cov_A[:10]))\n",
    "    if simu:\n",
    "        print('\\nGains fake:\\n', np.round(gains_fake[:10], 4))\n",
    "\n",
    "\n",
    "    cmap_bwr=flib.make_cmap_nan_black('bwr')\n",
    "    allmean_pull, allstd_pull = [], []\n",
    "    for k in range(nimages):\n",
    "        _, _, pull = get_pull(fringes[k], errs[k], A, PowerPhi[k])\n",
    "        allmean_pull.append(np.mean(pull))\n",
    "        allstd_pull.append(np.std(pull))\n",
    "    \n",
    "    condition = np.any([np.abs(i - 1) > 0.5 for i in allstd_pull])\n",
    "\n",
    "for k in range(nimages):\n",
    "    plot_residuals(q, xONAFP, yONAFP, BLs[k], fringes[k], the_mask, PowerPhi[k], errs[k], A, \n",
    "                   cmap=cmap_bwr, normalize=True, vmin=-1, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1402a31",
   "metadata": {},
   "source": [
    "### Correct images not used by the intercalibrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602ecf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_used = list(np.arange(12))\n",
    "for i in myselection[::-1]:\n",
    "    del not_used[i]\n",
    "print(myselection, not_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86812c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if selection:\n",
    "    fringes_not_used = [remind_all_fringes[i] for i in not_used]\n",
    "    \n",
    "    for k in range(len(not_used)):\n",
    "        bl = fdict['BLS'][not_used[k]]\n",
    "        correct = fringes_not_used[k] / A\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2)\n",
    "        fig.suptitle(f'BL {bl}')\n",
    "        flib.plot_fringes_scatter(q, xONAFP, yONAFP, fringes_not_used[k]*the_mask, fig=fig, ax=axs[0], frame='ONAFP', \n",
    "                         title=f'Data', s=190, normalize=True)\n",
    "        flib.plot_fringes_scatter(q, xONAFP, yONAFP, correct*the_mask, fig=fig, ax=axs[1], frame='ONAFP', \n",
    "                         title=f'Data / gains', s=190, normalize=True)\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c201da1",
   "metadata": {},
   "source": [
    "## MCMC\n",
    "\n",
    "To get errors on the focal length, theta and P_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb80514",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "ncpu = cpu_count()\n",
    "print(\"{0} CPUs\".format(ncpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8328e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnlike(params, allInvCov, alldata, BLs):\n",
    "    chi2, A, _ = scal.get_chi2(params, allInvCov, alldata, BLs, q, returnA=True)\n",
    "    LnLike = - 0.5 * chi2\n",
    "#     print('A:', A[:20])\n",
    "#     print('chi2:', chi2)\n",
    "    return LnLike, A\n",
    "\n",
    "def lnprior(params):\n",
    "    fl = params[0]\n",
    "    th = params[1]\n",
    "    logP = params[2:]\n",
    "\n",
    "    if fl > 0.:\n",
    "        return 0.\n",
    "    return -np.inf\n",
    "    \n",
    "\n",
    "def lnprob(params, allInvCov, alldata, BLs):\n",
    "    lp = lnprior(params)\n",
    "#     print('Prior', lp)\n",
    "    ndet = alldata[0].shape[0]\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf, np.ones(248)*(-np.inf)\n",
    "    LnLike, A = lnlike(params, allInvCov, alldata, BLs)\n",
    "    return lp + LnLike, A\n",
    "\n",
    "\n",
    "class MCMC:\n",
    "    def __init__(self, nwalkers, niter, ndim, p0, burnin, axis_names, withpool=False, emcee_filename='emcee.h5'):\n",
    "        self.nwalkers = nwalkers\n",
    "        self.niter = niter\n",
    "        self.ndim = ndim\n",
    "        self.p0 = p0\n",
    "        self.burnin = burnin\n",
    "        self.axis_names = axis_names\n",
    "        self.withpool = withpool\n",
    "        self.emcee_filename = emcee_filename\n",
    "   \n",
    "\n",
    "    def run(self, lnprob, args, backend=True):\n",
    "        \n",
    "        with Pool() as pool:\n",
    "            if not self.withpool:\n",
    "                pool = None\n",
    "            if backend:\n",
    "                backend = emcee.backends.HDFBackend(self.emcee_filename)\n",
    "     \n",
    "                sampler = emcee.EnsembleSampler(self.nwalkers, self.ndim, lnprob, args=args, \n",
    "                                                pool=pool,\n",
    "                                                backend=backend)\n",
    "                if backend.iteration > 0:\n",
    "                    self.p0 = backend.get_last_sample()\n",
    "                if self.niter - backend.iteration > 0:\n",
    "                    print(\"\\n =========== Running production... ===========\")\n",
    "                    start = time.time()\n",
    "                    sampler.run_mcmc(self.p0, \n",
    "                                     nsteps=max(0, self.niter - backend.iteration), \n",
    "                                     progress=True)   \n",
    "            else:\n",
    "                sampler = emcee.EnsembleSampler(self.nwalkers, self.ndim, lnprob, args=args, \n",
    "                                                pool=pool,\n",
    "                                                backend=None)\n",
    "                print(\"\\n =========== Running production... ===========\")\n",
    "                start = time.time()\n",
    "                sampler.run_mcmc(self.p0, nsteps=self.niter, progress=True)\n",
    "                \n",
    "        end = time.time()\n",
    "        print(\"MCMC took {0:.1f} seconds\".format(end - start))       \n",
    "        return sampler\n",
    "    \n",
    "    def read_sampler(self, sampler, has_blobs=True):\n",
    "        if has_blobs:\n",
    "            self.blobs = sampler.get_blobs(flat=False)\n",
    "        \n",
    "        self.chains = sampler.get_chain(discard=0, flat=False, thin=1)\n",
    "        self.lnprobs = sampler.get_log_prob(discard=0, flat=False, thin=1)\n",
    "        \n",
    "\n",
    "    def read_backends(self):\n",
    "        reader = emcee.backends.HDFBackend(self.emcee_filename)\n",
    "        try:\n",
    "            tau = reader.get_autocorr_time()\n",
    "        except emcee.autocorr.AutocorrError:\n",
    "            tau = -1\n",
    "        self.tau = tau\n",
    "        if reader.has_blobs():\n",
    "            self.blobs = reader.get_blobs(flat=False)\n",
    "        self.chains = reader.get_chain(discard=0, flat=False, thin=1)\n",
    "        self.lnprobs = reader.get_log_prob(discard=0, flat=False, thin=1)\n",
    "        return\n",
    "    \n",
    "    def compute_local_acceptance_rate(self, start_index, last_index, walker_index):\n",
    "        \"\"\"Compute the local acceptance rate in a chain.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_index: int\n",
    "            Beginning index.\n",
    "        last_index: int\n",
    "            End index.\n",
    "        walker_index: int\n",
    "            Index of the walker.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        freq: float\n",
    "            The acceptance rate.\n",
    "\n",
    "        \"\"\"\n",
    "        frequences = []\n",
    "        test = -2 * self.lnprobs[start_index, walker_index]\n",
    "        counts = 1\n",
    "        for index in range(start_index + 1, last_index):\n",
    "            chi2 = -2 * self.lnprobs[index, walker_index]\n",
    "            if np.isclose(chi2, test):\n",
    "                counts += 1\n",
    "            else:\n",
    "                frequences.append(float(counts))\n",
    "                counts = 1\n",
    "                test = chi2\n",
    "        frequences.append(counts)\n",
    "        return 1.0 / np.mean(frequences)\n",
    "\n",
    "    def set_chain_validity(self):\n",
    "        \"\"\"Test the validity of a chain: reject chains whose chi2 is far from the mean of the others.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        valid_chains: list\n",
    "            List of boolean values, True if the chain is valid, or False if invalid.\n",
    "\n",
    "        \"\"\"\n",
    "        nchains = [k for k in range(self.nwalkers)]\n",
    "        chisq_averages = []\n",
    "        chisq_std = []\n",
    "        for k in nchains:\n",
    "            chisqs = -2 * self.lnprobs[self.burnin:, k]\n",
    "            chisq_averages.append(np.mean(chisqs))\n",
    "            chisq_std.append(np.std(chisqs))\n",
    "        self.global_average = np.mean(chisq_averages)\n",
    "        self.global_std = np.mean(chisq_std)\n",
    "        self.valid_chains = [False] * self.nwalkers\n",
    "        for k in nchains:\n",
    "            chisqs = -2 * self.lnprobs[self.burnin:, k]\n",
    "            chisq_average = np.mean(chisqs)\n",
    "            chisq_std = np.std(chisqs)\n",
    "            if 3 * self.global_std + self.global_average < chisq_average:\n",
    "                self.valid_chains[k] = False\n",
    "            elif chisq_std < 0.1 * self.global_std:\n",
    "                self.valid_chains[k] = False\n",
    "            else:\n",
    "                self.valid_chains[k] = True\n",
    "        return self.valid_chains\n",
    "    \n",
    "    def get_reduce_chi2(self, nimages, ndet):\n",
    "        NDDL = (nimages * ndet -(ndet + nimages + 2))\n",
    "        return self.global_average / NDDL\n",
    "    \n",
    "    def plot_chains_chi2(self, fontsize=14):\n",
    "        \"\"\"Plot chains and chi2.\"\"\"\n",
    "        chains = self.chains[self.burnin:, :, :]  # .reshape((-1, self.ndim))\n",
    "        nchains = [k for k in range(self.nwalkers)]\n",
    "        steps = np.arange(self.burnin, self.niter)\n",
    "        \n",
    "        fig, ax = plt.subplots(self.ndim + 1, 1, figsize=(10, 7), sharex='all')\n",
    "        \n",
    "        # Chi2 vs Index\n",
    "        print(\"Chisq statistics:\")\n",
    "        for k in nchains:\n",
    "            chisqs = -2 * self.lnprobs[self.burnin:, k]\n",
    "            text = f\"\\tWalker {k:d}: {float(np.mean(chisqs)):.3f} +/- {float(np.std(chisqs)):.3f}\"\n",
    "            if not self.valid_chains[k]:\n",
    "                text += \" -> excluded\"\n",
    "                ax[self.ndim].plot(steps, chisqs, c='0.5', linestyle='--')\n",
    "            else:\n",
    "                ax[self.ndim].plot(steps, chisqs)\n",
    "            print(text)\n",
    "        \n",
    "        ax[self.ndim].set_ylim(\n",
    "            [self.global_average - 5 * self.global_std, self.global_average + 5 * self.global_std])\n",
    "        \n",
    "        # Parameter vs Index\n",
    "        print(\"Computing Parameter vs Index plots...\")\n",
    "        for i in range(self.ndim):\n",
    "            h = ax[i].set_ylabel(self.axis_names[i], fontsize=fontsize)\n",
    "            h.set_rotation(0)\n",
    "            for k in nchains:\n",
    "                if self.valid_chains[k]:\n",
    "                    ax[i].plot(steps, chains[:, k, i])\n",
    "                else:\n",
    "                    ax[i].plot(steps, chains[:, k, i], c='0.5', linestyle='--')\n",
    "                ax[i].get_yaxis().set_label_coords(-0.05, 0.5)\n",
    "        h = ax[self.ndim].set_ylabel(r'$\\chi^2$', fontsize=fontsize)\n",
    "        h.set_rotation(0)\n",
    "        ax[self.ndim].set_xlabel('Steps', fontsize=fontsize)\n",
    "        ax[self.ndim].get_yaxis().set_label_coords(-0.05, 0.5)\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(hspace=0)\n",
    "        figure_name = self.emcee_filename.replace('.h5', '_chains.pdf')\n",
    "        print(f'Save figure: {figure_name}')\n",
    "        fig.savefig(figure_name, dpi=100)\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "    def convergence_tests(self, fontsize=14):\n",
    "        \"\"\"\n",
    "        Compute the convergence tests (Gelman-Rubin, acceptance rate).\n",
    "\n",
    "        \"\"\"\n",
    "        chains = self.chains[self.burnin:, :, :]\n",
    "        nchains = [k for k in range(self.nwalkers)]\n",
    "        steps = np.arange(self.burnin, self.niter)\n",
    "        \n",
    "        # Acceptance rate vs Index\n",
    "        print(\"Computing acceptance rate...\")\n",
    "        min_len = self.niter\n",
    "        window = 100\n",
    "        \n",
    "        fig, ax = plt.subplots(self.ndim + 1, 1, figsize=(10, 7), sharex='all')\n",
    "        print(ax.shape)\n",
    "        if min_len > window:\n",
    "            for k in nchains:\n",
    "                ARs = []\n",
    "                indices = []\n",
    "                for pos in range(self.burnin + window, self.niter, window):\n",
    "                    ARs.append(self.compute_local_acceptance_rate(pos - window, pos, k))\n",
    "                    indices.append(pos)\n",
    "                if self.valid_chains[k]:\n",
    "                    ax[self.ndim].plot(indices, ARs, label=f'Walker {k:d}')\n",
    "                else:\n",
    "                    ax[self.ndim].plot(indices, ARs, label=f'Walker {k:d}', c='gray', linestyle='--')\n",
    "                ax[self.ndim].set_xlabel('Steps', fontsize=fontsize)\n",
    "                ax[self.ndim].set_ylabel('Aceptance rate', fontsize=8)\n",
    "        \n",
    "        # Gelman-Rubin test\n",
    "        if len(nchains) > 1:\n",
    "            step = max(1, (self.niter - self.burnin) // 20)\n",
    "            self.gelmans = []\n",
    "            print(f'Gelman-Rubin tests (burnin={self.burnin:d}, step={step:d}, nsteps={self.niter:d}):')\n",
    "            for i in range(self.ndim):\n",
    "                Rs = []\n",
    "                lens = []\n",
    "                for pos in range(self.burnin + step, self.niter, step):\n",
    "                    chain_averages = []\n",
    "                    chain_variances = []\n",
    "                    global_average = np.mean(self.chains[self.burnin:pos, self.valid_chains, i])\n",
    "                    for k in nchains:\n",
    "                        if not self.valid_chains[k]:\n",
    "                            continue\n",
    "                        chain_averages.append(np.mean(self.chains[self.burnin:pos, k, i]))\n",
    "                        chain_variances.append(np.var(self.chains[self.burnin:pos, k, i], ddof=1))\n",
    "                    W = np.mean(chain_variances)\n",
    "                    B = 0\n",
    "                    for n in range(len(chain_averages)):\n",
    "                        B += (chain_averages[n] - global_average) ** 2\n",
    "                    B *= ((pos + 1) / (len(chain_averages) - 1))\n",
    "                    R = (W * pos / (pos + 1) + B / (pos + 1) * (len(chain_averages) + 1) / len(chain_averages)) / W\n",
    "                    Rs.append(R - 1)\n",
    "                    lens.append(pos)\n",
    "#                 print(f'\\t{self.input_labels[i]}: R-1 = {Rs[-1]:.3f} (l = {lens[-1] - 1:d})')\n",
    "                self.gelmans.append(Rs[-1])\n",
    "                ax[i].plot(lens, Rs, lw=1, label=self.axis_names[i])\n",
    "                ax[i].axhline(0.03, c='k', linestyle='--')\n",
    "                ax[i].set_xlabel('Walker length', fontsize=fontsize)\n",
    "                ax[i].set_ylabel('$R-1$', fontsize=fontsize)\n",
    "                ax[i].set_ylim(0, 0.6)\n",
    "                # ax[self.dim, 3].legend(loc='best', ncol=2, fontsize=10)\n",
    "        self.gelmans = np.array(self.gelmans)\n",
    "        fig.tight_layout()\n",
    "        plt.subplots_adjust(hspace=0)\n",
    "        figure_name = self.emcee_filename.replace('.h5', '_convergence.pdf')\n",
    "        print(f'Save figure: {figure_name}')\n",
    "        fig.savefig(figure_name, dpi=100)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_params_errors(self):\n",
    "        chains = self.chains[self.burnin:, self.valid_chains, :] \n",
    "        \n",
    "        self.params = np.mean(chains, axis=(0, 1))\n",
    "        self.params_std = np.std(chains, axis=(0, 1))\n",
    "        \n",
    "        for i in range(ndim):\n",
    "            print(f'***** {self.axis_names[i]}:')\n",
    "            if i == 1:\n",
    "                print(f'{np.rad2deg(mcmc.params[i]):.5f} +/- {np.rad2deg(mcmc.params_std[i]):.5f}')\n",
    "            else:\n",
    "                print(f'{mcmc.params[i]:.5f} +/- {mcmc.params_std[i]:.5f}')\n",
    "        \n",
    "        s, m, p, = np.shape(chains)\n",
    "        flat_chains = np.reshape(chains, (s*m, p))\n",
    "        self.params_cov = np.cov(flat_chains.T)\n",
    "        print(self.params_cov.shape)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "myselection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b286ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pk_minuit\n",
    "LogPk_minuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba99a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_guess = [fl_minuit, th_minuit] + list(LogPk_minuit)\n",
    "# params_guess = [0.3, np.deg2rad(0.5), 0.5, 0.5, 0.5]\n",
    "print('Guess:', params_guess)\n",
    "\n",
    "ndim = len(params_guess)\n",
    "nwalkers = 30\n",
    "\n",
    "# Initial guess\n",
    "p0 = [params_guess + 1e-4 * np.random.rand(ndim) for i in range(nwalkers)]\n",
    "# print(p0)\n",
    "\n",
    "niter = 3000\n",
    "axis_names = ['Focal', 'Theta'] + [f'LogP{i+1}' for i in range(nimages)]\n",
    "\n",
    "mcmc = MCMC(nwalkers, niter, ndim, p0, burnin=0, axis_names=axis_names, withpool=False, \n",
    "            emcee_filename='emcee_2020-10-27_BLs1-2-4-5-7-8-9-10-11_v1.h5')\n",
    "args = (allInvCov, fringes, BLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the MCMC and save a backend\n",
    "sampler = mcmc.run(lnprob, args, backend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90766039",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.read_backends()\n",
    "chain_validity = mcmc.set_chain_validity()\n",
    "print(np.sum(chain_validity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b54ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.plot_chains_chi2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aebfba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.convergence_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8873b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_reduce = mcmc.get_reduce_chi2(nimages, ndet)\n",
    "\n",
    "print(f'Reduce Chi2: {chi2_reduce:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.get_params_errors()\n",
    "from qubic.AnalysisMC import cov2corr\n",
    "# Covariance between parameters\n",
    "lim = 1#np.abs(np.max(mcmc.params_cov)/100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "im = ax.imshow(cov2corr(mcmc.params_cov), cmap='bwr', vmin=-lim, vmax=lim)\n",
    "ax.set_xticks(np.arange(nimages+2))\n",
    "ax.set_yticks(np.arange(nimages+2))\n",
    "ax.set_xticklabels(mcmc.axis_names)\n",
    "ax.set_yticklabels(mcmc.axis_names)\n",
    "ax.set_title('Correlation')\n",
    "fig.colorbar(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getdist\n",
    "from getdist import plots, MCSamples\n",
    "\n",
    "mcmc.burnin = 0\n",
    "\n",
    "labels = ['f', r'\\theta'] + [f'Log(P_{i+1})' for i in range(nimages)]\n",
    "samps = MCSamples(samples=mcmc.chains[mcmc.burnin:], names=mcmc.axis_names, labels=labels,\n",
    "        ranges={'Focal':(0, None), 'Theta':(None,None)})\n",
    "\n",
    "g = plots.getSubplotPlotter()\n",
    "g.triangle_plot(samps, filled=True, title_limit=2, \n",
    "                markers=params_fake)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed2d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(mcmc.chains[:,0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b71ae3",
   "metadata": {},
   "source": [
    "#### Get A from the fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84aa27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing params with the covariance found with the MCMC\n",
    "# The distribution is considered as gaussian\n",
    "size = 1000\n",
    "distrib = np.random.multivariate_normal(mcmc.params, mcmc.params_cov, size=size)\n",
    "allA = np.zeros((size, ndet))\n",
    "for i in range(size):\n",
    "    params = distrib[i]\n",
    "    q.optics.focal_length = mcmc.params[0]\n",
    "    allPowerPhi = []\n",
    "    for k in range(nimages):\n",
    "        model = scal.Model_Fringes_Ana(q, BLs[k], \n",
    "                                        theta_source=mcmc.params[1], \n",
    "                                        nu_source=150e9, \n",
    "                                        fwhm=20., amp=1., frame='ONAFP')\n",
    "\n",
    "        x, y, Phi = model.get_fringes(times_gaussian=False)\n",
    "        \n",
    "        # Global amplitude\n",
    "        allPowerPhi.append(Phi * 10**mcmc.params[2+k])\n",
    "\n",
    "\n",
    "    # Gain for each detector\n",
    "    allA[i, :], Cov_A = scal.get_gains(allPowerPhi, allInvCov, fringes)\n",
    "\n",
    "mcmcA_std = np.std(allA, axis=0)    \n",
    "mcmcA = np.mean(allA, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get A from the blob (A computed and saved during the MCMC) \n",
    "blobA = mcmc.blobs[mcmc.burnin:, :, :]\n",
    "blobA = np.reshape(blobA, ((mcmc.niter-mcmc.burnin)*mcmc.nwalkers, 248))\n",
    "\n",
    "# Mean and STD along the chain\n",
    "blobA_std = np.nanstd(blobA, axis=0)\n",
    "print(blobA_std.shape)\n",
    "\n",
    "blobA_mean = np.nanmean(blobA, axis=0)\n",
    "# print(blobA_man)\n",
    "\n",
    "\n",
    "Cov_A = np.cov(blobA.T)\n",
    "# print(Cov_A)\n",
    "weights = 1 / np.diag(Cov_A)\n",
    "weights = np.nan_to_num(weights, nan=1e-10)\n",
    "avgA = np.average(np.nan_to_num(blobA_mean, nan=1), weights=weights)\n",
    "print(avgA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(np.min(gains_fake), np.max(gains_fake), 0.5)\n",
    "if simu:\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(13, 13))\n",
    "    fig.subplots_adjust(wspace=0.3)\n",
    "    ax1, ax2, ax3, ax4 = np.ravel(axs)\n",
    "    ax1.errorbar(allP_fake, mean_param[2:], yerr=std_param[2:], fmt='o', color='r', label='Mean, STD')\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', label='y=x')\n",
    "    ax1.set_xlabel('P Fake Data')\n",
    "    ax1.set_ylabel('Fit result')\n",
    "    ax1.set_title('Power Pk')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.errorbar(gains_fake, A, yerr=np.sqrt(np.diag(Cov_A)), fmt='o', color='b', label='A, CovA')\n",
    "    ax2.plot(xx, xx, 'k--', label='y=x')\n",
    "    ax2.set_xlabel('Gain Fake Data')\n",
    "#     ax2.set_ylabel('Gain Fit result')\n",
    "    ax2.set_title('Gain')\n",
    "    ax2.legend()\n",
    "    \n",
    "    ax3.errorbar(gains_fake, meanA, yerr=stdA, fmt='o', color='g', label='mean, STD')\n",
    "    ax3.plot(xx, xx, 'k--', label='y=x')\n",
    "    ax3.set_xlabel('Gain Fake Data')\n",
    "#     ax3.set_ylabel('Gain with Monte Carlo')\n",
    "    ax3.set_title('Gain with MC')\n",
    "    ax3.legend()\n",
    "    \n",
    "#     ax4.errorbar(gains_fake, blobA_mean, yerr=blobA_std, fmt='o', color='r', label='mean, STD')\n",
    "#     ax4.plot(xx, xx, 'k--', label='y=x')\n",
    "#     ax4.set_xlabel('Gain Fake Data')\n",
    "# #     ax4.set_ylabel('Gain with Monte Carlo')\n",
    "#     ax4.set_title('Gains from blob')\n",
    "#     ax4.legend()\n",
    "#     fig.tight_layout()\n",
    "\n",
    "else:\n",
    "    vmin=-5\n",
    "    vmax = 5\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(13, 10))\n",
    "    fig.suptitle('Gains and errors found with MCMC')\n",
    "    ax1, ax2, ax3, ax4 = np.ravel(axs)\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, blobA_mean*the_mask, \n",
    "                              fig=fig, ax=ax1, title='Gains A', \n",
    "                              normalize=False, vmin=vmin, vmax=vmax, \n",
    "                              s=100)\n",
    "\n",
    "    flib.plot_fringes_scatter(q, xONAFP, yONAFP, blobA_std*the_mask, \n",
    "                              fig=fig, ax=ax2, title='Errors STD(A)', \n",
    "                              normalize=False, vmin=vmin, vmax=vmax, \n",
    "                              s=100)\n",
    "    \n",
    "    ax3.hist(blobA_mean, bins=30, range=(-10, 10))\n",
    "    ax3.set_xlabel('Gains found with MCMC')\n",
    "#     ax3.axvline(np.nanmean(blobA_mean), color='r')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.errorbar(A, meanA, xerr=np.sqrt(np.diag(Cov_A)), yerr=stdA, fmt='o')\n",
    "#     plt.plot(A, A, label='y=x')\n",
    "#     plt.xlabel('A')\n",
    "#     plt.ylabel('Mean A after MC')\n",
    "# #     plt.axis('equal')\n",
    "#     plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433f87e",
   "metadata": {},
   "source": [
    "#### Residuals and data corrected by inter-calibrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_viridis = flib.make_cmap_nan_black('viridis')\n",
    "for k in range(nimages):\n",
    "    plot_residuals(q, xONAFP, yONAFP, BLs[k], \n",
    "                   fringes[k], \n",
    "                   the_mask, \n",
    "                   allPowerPhi[k], \n",
    "                   errs[k], \n",
    "                   blobA_mean, cmap=cmap_viridis,\n",
    "                   normalize=True,\n",
    "                   vmin=-1, vmax=1,\n",
    "                   s=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4e170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
