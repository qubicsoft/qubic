{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077df225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "from matplotlib import rc\n",
    "rc('figure',figsize=(16,8))\n",
    "rc('font',size=12)\n",
    "rc('text',usetex=False)\n",
    "\n",
    "from qubicpack import qubicpack as qp\n",
    "import fibtools as ft\n",
    "import plotters as p\n",
    "import lin_lib as ll\n",
    "import demodulation_lib as dl\n",
    "\n",
    "from pysimulators import FitsArray\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "import matplotlib.mlab as mlab\n",
    "import scipy.ndimage.filters as f\n",
    "import glob\n",
    "import string\n",
    "import scipy.signal as scsig\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce9bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "014031eb",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Between March 13th 2019 and March 17th 2019 we have performed 4 series of scans, each starting at -XX in elevation (w.r.t. rest position==50 deg.) and then making 40 azimuth scans, each from -20 to 20 degrees. After each azimuth scan, the elevation is increased by 1 degree (not encoder degrees, only step motors, so with a significant uncertainty, but Louise has done some calibration of this).\n",
    "\n",
    "Here is a description of each dataset:\n",
    "1. \"ScanMap\": from 2019-03-13 @ 19h21 to 2019-03-14 @ 11h03 \n",
    "    - First scan from -20 in elevation to +20, therefore actual elevation from 30 to 70\n",
    "    - To be analyzed\n",
    "2. \"ScanMapNew\": from 2019-03-14 @ 13h22 to 15h34, then 2019-03-15 @ 13h42 to 14h13\n",
    "    - Many GPS issues with this scan\n",
    "    - finally interrupted. \n",
    "    - Not to be analaysed in priority\n",
    "3. \"ScanMapNew2\": from 2019-03-15 @ 17h21 to 2019-03-16 @ 9h17\n",
    "    - Scan from -20 in elevation to +20, therefore actual elevation from 30 to 70\n",
    "    - Cycle finished at scan 38 or 39 => take care of this\n",
    "    - to be analysed\n",
    "4. \"ScanMapNew2_Start_40.5\": from 2019-03-16 @ 20h17 to 2019-03-17 @ 12h15\n",
    "    - Scan started at el-19.5 to + 20.5: therefore actual elevation 30.5 to 70.5\n",
    "    - to be analyzed\n",
    "    \n",
    "Lets get the directories corresponding to each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1004fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['ScanMap', 'ScanMapNew2', 'ScanMapNew2_start-40.5']\n",
    "days = [['2019-03-13', '2019-03-14'], ['2019-03-15', '2019-03-16'], ['2019-03-16', '2019-03-17']]\n",
    "el_start = [30., 30., 30.5]\n",
    "delta_el = 1.\n",
    "\n",
    "all_elevation = []\n",
    "datasets=[]\n",
    "for inames in xrange(len(names)):\n",
    "    n = names[inames]\n",
    "    print n, ' Elevation starts at {}'.format(el_start[inames])\n",
    "    datasets.append([])\n",
    "    for d in days[inames]:\n",
    "        dd = glob.glob('/qubic/Data/Calib-TD/'+d+'/*'+n)\n",
    "        for i in xrange(len(dd)): \n",
    "            datasets[inames].append(dd[i])\n",
    "        print '  * ',d,' : {} files'.format(len(dd))\n",
    "    print '  => Total = {} files'.format(len(datasets[inames]))\n",
    "    elevations = el_start[inames]+arange(len(datasets[inames]))*delta_el\n",
    "    all_elevation.append(elevations)\n",
    "    print '  => Elevation ends at {}'.format(np.max(elevations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca99efe7",
   "metadata": {},
   "source": [
    "We start with the forst dataset ('ScanMap'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "dirs = datasets[index]\n",
    "elevation = all_elevation[index]\n",
    "\n",
    "labels = []\n",
    "dir_time = []\n",
    "for d in dirs:\n",
    "    bla = str.split(d,'__')\n",
    "    blo = str.split(bla[0],'/')\n",
    "    labels.append(bla[1])\n",
    "    dir_time.append(blo[-1])\n",
    "    \n",
    "for i in xrange(len(labels)): \n",
    "    print labels[i], dir_time[i], 'Elevation: ', elevation[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb2810",
   "metadata": {},
   "source": [
    "## Reading TES data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "write_dir = '/Volumes/Data/Qubic/Calib-TD/ScansNightDay/'\n",
    "\n",
    "AsicNum = 1\n",
    "TESnum = 96\n",
    "\n",
    "for idir in xrange(len(dirs)):\n",
    "    print '################## Dir {} / {} ################'.format(idir,len(dirs))\n",
    "    thedir = dirs[idir]\n",
    "\n",
    "    #### Read data\n",
    "    a = qp()\n",
    "    a.read_qubicstudio_dataset(thedir, asic=AsicNum)\n",
    "    TESNum = 96\n",
    "    data = a.timeline(TES=TESNum)\n",
    "    t_data = a.timeline_timeaxis(axistype='pps')\n",
    "\n",
    "    #### Power Spectrum\n",
    "    FREQ_SAMPLING = 1./(t_data[1]-t_data[0])\n",
    "    spectrum_f, freq_f = mlab.psd(data, Fs=FREQ_SAMPLING, NFFT=len(data)/10, window=mlab.window_hanning)\n",
    "\n",
    "    FitsArray(freq_f).save(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_PS_freq.fits')\n",
    "    FitsArray(spectrum_f).save(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_PS_spec.fits')\n",
    "\n",
    "    #### Time/Freq Analysis\n",
    "    f, t, Sxx = scipy.signal.spectrogram(data, fs=FREQ_SAMPLING, window=('tukey', 0.25), \n",
    "                                   nperseg=1024, noverlap=None, nfft=None, \n",
    "                                   detrend='constant', return_onesided=True, \n",
    "                                   scaling='density', axis=-1, mode='psd')\n",
    "\n",
    "    FitsArray(f).save(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_freq.fits')\n",
    "    FitsArray(t).save(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_time.fits')\n",
    "    FitsArray(Sxx).save(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_spec.fits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c66e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fini !')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84199b3e",
   "metadata": {},
   "source": [
    "### Now Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7631f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allf = []\n",
    "allspec = []\n",
    "allf2d = []\n",
    "allt2d = []\n",
    "alltf = []\n",
    "for idir in xrange(len(dirs)):\n",
    "    allf.append(FitsArray(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_PS_freq.fits'))\n",
    "    allspec.append(FitsArray(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_PS_spec.fits'))    \n",
    "    allf2d.append(FitsArray(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_freq.fits'))\n",
    "    allt2d.append(FitsArray(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_time.fits'))\n",
    "    alltf.append(FitsArray(write_dir+labels[idir]+'_'+str(100+idir)+'_'+dir_time[idir]+'_TimeFreq_spec.fits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_img = np.zeros((len(dirs), 10000))\n",
    "valfreq = np.linspace(0, FREQ_SAMPLING/2, 10000)\n",
    "valfreq = np.logspace(-1, np.log10(FREQ_SAMPLING/2), 10000)\n",
    "\n",
    "for i in xrange(len(dirs)):\n",
    "    plot(allf[i], allspec[i], label=dir_time[i], color=cm.rainbow(i*1./len(dirs)))\n",
    "    spec_img[i,:] = np.interp(valfreq, allf[i], allspec[i])\n",
    "    \n",
    "xscale('log')\n",
    "yscale('log')\n",
    "legend(fontsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.linspace(19.5, 11.+24, 41)\n",
    "pcolormesh(np.log10(valfreq), time, np.log10(spec_img))\n",
    "xlabel('Log(freq)')\n",
    "ylabel('local time')\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c256a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ii=20\n",
    "pcolormesh(np.array(allf2d[ii]), np.array(allt2d[ii]), np.log10(np.array(alltf[ii].T)))\n",
    "colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3907618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213d80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1253eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
