{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfccafea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.constants import c\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3642d691",
      "metadata": {},
      "source": [
        "# Import Pickle Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb24bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"GPS_fit_results.pkl\", \"rb\") as f:\n",
        "    dict_fit = pickle.load(f)\n",
        "\n",
        "data_names = dict_fit[\"values\"].keys()\n",
        "parameters_names = dict_fit[\"values\"][\"rpN\"].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efa912a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"GPS_noise_analysis.pkl\", \"rb\") as f:\n",
        "    dict_data = pickle.load(f)\n",
        "\n",
        "rpN_data = dict_data[\"rpN\"][\"clean_data\"]\n",
        "rpE_data = dict_data[\"rpE\"][\"clean_data\"]\n",
        "rpD_data = dict_data[\"rpD\"][\"clean_data\"]\n",
        "roll_data = dict_data[\"roll\"][\"clean_data\"]\n",
        "yaw_data = dict_data[\"yaw\"][\"clean_data\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d917b4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "config_index = 3\n",
        "config_indices = [12, 3, 37]\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f219453",
      "metadata": {},
      "outputs": [],
      "source": [
        "def noise_model(x, A_white, f_knee, alpha, f_min=1e-6):\n",
        "    f = np.maximum(np.abs(x), f_min)\n",
        "    return A_white**2 * (1.0 + (f_knee / f) ** alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c0ba36",
      "metadata": {},
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f6c1fe",
      "metadata": {},
      "source": [
        "## Compute Covariance Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2be6993",
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.array([rpN_data[config_index], rpE_data[config_index], rpD_data[config_index], roll_data[config_index], yaw_data[config_index]])\n",
        "std_ = np.std(data, axis=1)\n",
        "cov = np.cov(data, rowvar=True)\n",
        "corr = np.corrcoef(data, rowvar=True)\n",
        "\n",
        "plt.imshow(cov)\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(len(data_names)), data_names, rotation=45)\n",
        "plt.yticks(np.arange(len(data_names)), data_names)\n",
        "plt.title(\"Covariance matrix of the GPS data\")\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(corr, vmin=-1, vmax=1, cmap=\"bwr\")\n",
        "plt.colorbar()\n",
        "plt.xticks(np.arange(len(data_names)), data_names, rotation=45)\n",
        "plt.yticks(np.arange(len(data_names)), data_names)\n",
        "plt.title(\"Correlation matrix of the GPS data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4202df9a",
      "metadata": {},
      "source": [
        "## Test Uncertainties from STD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa3ee64",
      "metadata": {},
      "outputs": [],
      "source": [
        "std_rpN, std_rpE, std_rpD = dict_fit[\"standard_deviation\"][\"rpN\"][config_index], dict_fit[\"standard_deviation\"][\"rpE\"][config_index], dict_fit[\"standard_deviation\"][\"rpD\"][config_index]\n",
        "std_roll, std_yaw = dict_fit[\"standard_deviation\"][\"roll\"][config_index], dict_fit[\"standard_deviation\"][\"yaw\"][config_index]\n",
        "\n",
        "sigma_std_pos = np.sqrt(std_rpN**2 + std_rpE**2 + std_rpD**2)\n",
        "sigma_std_ang = np.sqrt(std_roll**2 + std_yaw**2)\n",
        "\n",
        "print(f\"RMS 3D-error = {sigma_std_pos:.3f} cm\")\n",
        "print(f\"RMS angular error = {sigma_std_ang:.3f} deg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb588f84",
      "metadata": {},
      "source": [
        "## Test Uncertainties from PS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2a11dc",
      "metadata": {},
      "source": [
        "### Test Normalization Numpy FFT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a159ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "white_noise = np.ones(1000)\n",
        "\n",
        "ps = np.fft.fft(white_noise)\n",
        "ps = np.abs(ps) ** 2\n",
        "ps = ps / len(ps)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(ps, label=\"Power Spectrum of White Noise\")\n",
        "plt.xlabel(\"Frequency (Hz)\")\n",
        "plt.ylabel(\"Power\")\n",
        "plt.title(\"Power Spectrum of White Noise\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "noise = np.fft.ifft(ps)\n",
        "noise = noise[1:]\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(noise.real, label=\"Inverse FFT of Power Spectrum\")\n",
        "plt.xlabel(\"Sample Index\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.title(\"Inverse FFT of Power Spectrum\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"Normalization Factor:\", np.mean(noise.real) / np.mean(white_noise))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62df80ad",
      "metadata": {},
      "source": [
        "### Test Uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "901a02d3",
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_rate = 8  # Hz\n",
        "timestep = 1 / sampling_rate  # Time step in seconds\n",
        "f_nyq = sampling_rate / 2  # Nyquist frequency\n",
        "f_min = 1 / (timestep * len(rpN_data[0]))  # Minimum frequency\n",
        "\n",
        "# Frequencies for the power spectrum\n",
        "freqs = np.linspace(f_min, f_nyq, 20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c61e54e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "sigma2 = np.empty((3, len(data_names)))\n",
        "std_data = np.empty((3, len(data_names)))\n",
        "\n",
        "for iconfig, config_index in enumerate(config_indices):\n",
        "    print(f\"\\nConfiguration Index: {config_index}\")\n",
        "    print(\"=\" * 60)\n",
        "    for i, data_name in enumerate(data_names):\n",
        "        A_white = dict_fit[\"values\"][data_name][\"A_white\"][config_index]\n",
        "        f_knee = dict_fit[\"values\"][data_name][\"f_knee\"][config_index]\n",
        "        alpha = dict_fit[\"values\"][data_name][\"alpha\"][config_index]\n",
        "\n",
        "        # Power spectrum calculation\n",
        "        S = A_white**2 * (1 + np.abs(f_knee / freqs) ** alpha)\n",
        "        # Integrate\n",
        "        sigma2_val = np.trapz(S, freqs)\n",
        "        std_val = np.sqrt(sigma2_val)\n",
        "\n",
        "        if data_name in [\"rpN\", \"rpE\", \"rpD\"]:\n",
        "            unit = \"m\"\n",
        "        elif data_name in [\"roll\", \"yaw\"]:\n",
        "            unit = \"deg\"\n",
        "        else:\n",
        "            unit = \"unknown\"\n",
        "\n",
        "        sigma2[iconfig, i] = sigma2_val\n",
        "        std_data[iconfig, i] = dict_fit['standard_deviation'][data_name][config_index] / 100\n",
        "\n",
        "        print(f\"{data_name:>6} | std (PS) = {std_val:.4e} {unit} | std = {std_data[iconfig, i]:.4e} {unit}\")\n",
        "\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f433cd99",
      "metadata": {},
      "outputs": [],
      "source": [
        "sigma_ps_pos = np.sqrt(np.sum(sigma2[:3]))\n",
        "sigma_ps_ang = np.sqrt(np.sum(sigma2[3:]))\n",
        "\n",
        "print(f\"Uncertainty for antenna 1 position at config {config_index}: {sigma_ps_pos * 100:.3f} cm\")\n",
        "print(f\"Uncertainty for antenna 2 angle at config {config_index}: {np.degrees(sigma_ps_ang):.3f} deg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fd556ee",
      "metadata": {},
      "source": [
        "## Test Correlated Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e901996",
      "metadata": {},
      "source": [
        "### White Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95983762",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_correlated_noise(corr_matrix, std_devs, n_samples):\n",
        "    C = np.diag(std_devs) @ corr_matrix @ np.diag(std_devs)\n",
        "\n",
        "    return np.random.multivariate_normal(\n",
        "        mean=np.zeros(C.shape[0]), \n",
        "        cov=C, \n",
        "        size=n_samples,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93b9ed4",
      "metadata": {},
      "outputs": [],
      "source": [
        "test_corr_noise = generate_correlated_noise(corr, std_, 5000).T\n",
        "print(test_corr_noise.shape)\n",
        "\n",
        "color = [\"red\", \"blue\", \"green\", \"black\", \"pink\"]\n",
        "\n",
        "for i, idata_name in enumerate(data_names):\n",
        "    if i<6:\n",
        "        plt.plot(test_corr_noise[i], label=idata_name + f\" : {round(np.std(test_corr_noise[i]), 4)}\", alpha=0.8, color=color[i])\n",
        "plt.legend()\n",
        "plt.title(\"Simulated Correlated Noise from Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, idata_name in enumerate(data_names):\n",
        "    if i<5:\n",
        "        axs[i].hist(test_corr_noise[i], bins=30, alpha=0.7, color=color[i])\n",
        "        axs[i].set_title(f\"Histogram of Simulated Correlated Noise - {idata_name}\")\n",
        "        axs[i].set_xlabel(\"Value\")\n",
        "        axs[i].set_ylabel(\"Counts\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d5c9331",
      "metadata": {},
      "source": [
        "### Power Spectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6846882",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_correlated_noise_from_ps(R, n_samples, measured_stds, fs=sampling_rate, eps_reg=1e-14):\n",
        "    n_data = R.shape[0]\n",
        "    N = n_samples\n",
        "    freqs = np.fft.fftfreq(N, 1.0/fs)\n",
        "    df = fs / N\n",
        "\n",
        "    # --- 1) compute model PSD on grid for each channel ---\n",
        "    S_model = np.zeros((n_data, N), dtype=float)\n",
        "    for i, name in enumerate(data_names):\n",
        "        fp = dict_fit[\"values\"][name]\n",
        "        f_abs = np.maximum(np.abs(freqs), f_min)   # avoid f=0 singularity\n",
        "        S_vals = noise_model(f_abs,\n",
        "                             A_white=fp[\"A_white\"][config_index],\n",
        "                             f_knee=fp[\"f_knee\"][config_index],\n",
        "                             alpha=fp[\"alpha\"][config_index])\n",
        "        S_vals = np.asarray(S_vals, dtype=float)\n",
        "\n",
        "        # guard against NaN/inf\n",
        "        S_vals[~np.isfinite(S_vals)] = 0.0\n",
        "        S_model[i, :] = S_vals\n",
        "\n",
        "    # theoretical model variances (before scaling)\n",
        "    var_model = np.sum(S_model, axis=1) * df        # length n_data\n",
        "\n",
        "    # target variances from measured std\n",
        "    target_var = np.asarray(measured_stds, dtype=float)**2\n",
        "\n",
        "    # compute per-channel scale factors (guard divide-by-zero)\n",
        "    scale = np.ones(n_data)\n",
        "    mask_nonzero = var_model > 0\n",
        "    scale[mask_nonzero] = target_var[mask_nonzero] / var_model[mask_nonzero]\n",
        "    # if some var_model==0 (odd), keep scale=1 (or set to large number if you want)\n",
        "    # apply scale to PSD: S_scaled = scale[:,None] * S_model\n",
        "    S_scaled = S_model * scale[:, np.newaxis]\n",
        "\n",
        "    # --- generate multivariate colored noise using full cross-spectral factorization ---\n",
        "    # 1) independent white noise\n",
        "    white = np.random.randn(n_data, N)\n",
        "    W = np.fft.fft(white, axis=1)\n",
        "\n",
        "    # ensure R is PSD\n",
        "    R = (np.array(R, float) + np.array(R, float).T) / 2.0\n",
        "    wR, VR = np.linalg.eigh(R)\n",
        "    wR = np.clip(wR, 0.0, None)\n",
        "    R = (VR * wR[np.newaxis, :]) @ VR.T\n",
        "\n",
        "    X = np.zeros_like(W, dtype=np.complex128)\n",
        "\n",
        "    for k in range(N):\n",
        "        # S_vec for this freq using S_scaled\n",
        "        S_vec = S_scaled[:, k]\n",
        "        # sqrt diagonal\n",
        "        sqrtS = np.sqrt(np.maximum(S_vec, 0.0))\n",
        "        D = np.diag(sqrtS)\n",
        "        S_mat = D @ R @ D\n",
        "        S_mat = (S_mat + S_mat.T.conj()) / 2.0\n",
        "        S_mat += eps_reg * np.eye(n_data)\n",
        "\n",
        "        # eigen-decomp (small matrix)\n",
        "        w, V = np.linalg.eigh(S_mat)\n",
        "        w = np.clip(w, 0.0, None)\n",
        "        sqrt_diag = np.sqrt(w * df * N + 0.0)\n",
        "        M = (V * sqrt_diag[np.newaxis, :]) @ V.conj().T\n",
        "\n",
        "        X[:, k] = M @ W[:, k]\n",
        "\n",
        "    x = np.fft.ifft(X, axis=1).real\n",
        "\n",
        "    return x\n",
        "\n",
        "def generate_correlated_noise_from_ps_test(corr_matrix, n_samples, std, fs=sampling_rate):\n",
        "    R = corr_matrix\n",
        "    \n",
        "    n_data = R.shape[0]\n",
        "    N = n_samples\n",
        "    freqs = np.fft.fftfreq(N, 1.0/fs)\n",
        "    df = fs / N\n",
        "    \n",
        "    # 1) Build white noise\n",
        "    white = np.random.randn(n_data, N)\n",
        "    W = np.fft.fft(white, axis=1)\n",
        "    \n",
        "    # 2) Build noise PSD\n",
        "    # Build PS\n",
        "    S_model = np.zeros((n_data, N), dtype=float)\n",
        "    for i, name in enumerate(data_names):\n",
        "        fp = dict_fit[\"values\"][name]        \n",
        "        S_model[i, :] = noise_model(freqs,\n",
        "                             A_white=fp[\"A_white\"][config_index],\n",
        "                             f_knee=fp[\"f_knee\"][config_index],\n",
        "                             alpha=fp[\"alpha\"][config_index])\n",
        "    \n",
        "    # Scale PS\n",
        "    var_model = np.sum(S_model, axis=1) * df\n",
        "    scale = (std ** 2) / var_model\n",
        "    S = S_model * scale[:, None]\n",
        "        \n",
        "    # 3) Per frequency shaping + correlation\n",
        "    # D = np.sqrt(S)\n",
        "    # S = D @ R @ D\n",
        "    # eigvals, eigvecs = np.linalg.eigh(S)\n",
        "    # X = eigvecs @ np.sqrt(eigvals) @ eigvecs.T\n",
        "    \n",
        "    X = np.zeros_like(W, dtype=np.complex128)\n",
        "    for k, f in enumerate(freqs):\n",
        "        D = np.diag(np.sqrt(S[:, k]))\n",
        "        S_mat = D @ R @ D\n",
        "        eigvals, eigvecs = np.linalg.eigh(S_mat)\n",
        "        M = (eigvecs * np.sqrt((eigvals) * df * N)[None, :]) @ eigvecs.T\n",
        "        X[:, k] = M @ W[:, k]\n",
        "        \n",
        "    # 4) Back to time domain\n",
        "    x = np.fft.ifft(X, axis=1).real\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aa0a1e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_noise = generate_correlated_noise_from_ps(corr, 5000, std_)\n",
        "\n",
        "for i, idata_name in enumerate(data_names):\n",
        "    if i<6:\n",
        "        plt.plot(corr_noise[i], label=idata_name + f\" : {round(np.std(corr_noise[i]), 4)}\", alpha=0.8, color=color[i])\n",
        "plt.legend()\n",
        "plt.title(\"Simulated Correlated Noise from Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
        "\n",
        "for i, idata_name in enumerate(data_names):\n",
        "    if i<5:\n",
        "        axs[i].hist(corr_noise[i], bins=30, alpha=0.7, color=color[i])\n",
        "        axs[i].set_title(f\"Histogram of Simulated Correlated Noise - {idata_name}\")\n",
        "        axs[i].set_xlabel(\"Value\")\n",
        "        axs[i].set_ylabel(\"Counts\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e0b9c3",
      "metadata": {},
      "source": [
        "# Calibration Source Beam from position and orientation uncertainties "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d5a8f1f",
      "metadata": {},
      "source": [
        "For this work, we don't want to perform accurate estimation but only order of magnitude.\n",
        "So, we will assume that the two antennas are perfectly aligned with the calibration source's line-of-sight, which is pointing exactly at the center of QUBIC, and considering that the system GPS + Calibration Source is perfectly fixed together.\n",
        "\n",
        "Under these hypotheses:\n",
        "* The uncertainty on the calibration source position is the uncertainty on the antenna 1,\n",
        "* The uncertainty on the angels between antennas 1 & 2 is the uncertainty on the calibration source orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e54f414",
      "metadata": {},
      "outputs": [],
      "source": [
        "qubic_height = 4  # m\n",
        "calsource_height = 48.6  # m\n",
        "ground_distance = 38.44  # m\n",
        "\n",
        "distance_qubic_calsource = np.sqrt((calsource_height - qubic_height) ** 2 + ground_distance**2)\n",
        "\n",
        "print(f\"Distance between QUBIC and calibration source: {distance_qubic_calsource:.2f} m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e704610",
      "metadata": {},
      "source": [
        "## Gaussian Beam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e63fb31",
      "metadata": {},
      "source": [
        "Spot size : $w(z) = w_0 \\sqrt{1 + (z/z_R)^2}$, where $z_R = \\pi w_0^2 n / \\lambda \\\\$\n",
        "\n",
        "Gaussian Beam Intensity : $\\\\$\n",
        "$I(r, z) = I_0 (\\frac{w_0}{w(z)})^2 exp(-\\frac{2r^2}{w^2(z)})$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed5bd52c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_beam(r, z, w0, I0=1.0):\n",
        "    \"\"\"\n",
        "    Calculate the Gaussian beam intensity at a distance r from the center\n",
        "    and at a distance z from the beam waist.\n",
        "\n",
        "    Parameters:\n",
        "    r : radial distance from the center (m)\n",
        "    z : distance from the beam waist (m)\n",
        "    w0 : beam waist radius (m)\n",
        "    I0 : peak intensity at the beam waist (W/m²)\n",
        "\n",
        "    Returns:\n",
        "    I : intensity at distance r and z (W/m²)\n",
        "    \"\"\"\n",
        "    z_R = np.pi * w0**2 / c  # Rayleigh range\n",
        "    w_z = w0 * np.sqrt(1 + (z / z_R) ** 2)  # Beam radius at distance z\n",
        "    return I0 * (w0 / w_z) ** 2 * np.exp(-2 * r**2 / w_z**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66225563",
      "metadata": {},
      "outputs": [],
      "source": [
        "freq = 150e9  # Frequency in Hz\n",
        "wavelength = c / freq  # Wavelength in meters\n",
        "FWHM = 13  # degrees\n",
        "FWHM_rad = np.radians(FWHM)\n",
        "\n",
        "# Calculate the beam waist radius w0\n",
        "theta_0 = FWHM_rad / (2 * np.sqrt(2 * np.log(2)))\n",
        "w0 = wavelength / (np.pi * theta_0)\n",
        "print(f\"Beam waist radius w0 = {w0:.3f} m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a55753",
      "metadata": {},
      "outputs": [],
      "source": [
        "r = np.linspace(1e-3, 50, 10000)\n",
        "z = distance_qubic_calsource.round(2)  # Distance from the beam waist\n",
        "I0 = 1.0\n",
        "\n",
        "beam = gaussian_beam(r, z, w0, I0)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(r, beam, label=f\"z = {z} m\")\n",
        "plt.xlabel(\"Radial distance r (m)\")\n",
        "plt.ylabel(\"Intensity I(r) (W/m²)\")\n",
        "plt.title(\"Radial intensity profile of Gaussian beam\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "r = np.linspace(1e-3, 500000, 10000)\n",
        "\n",
        "beam = gaussian_beam(r, z, w0, I0)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(r / 1000, beam, label=f\"z = {z} m\")\n",
        "plt.xlabel(\"Radial distance r (km)\")\n",
        "plt.ylabel(\"Intensity I(r) (W/m²)\")\n",
        "plt.title(\"Radial intensity profile of Gaussian beam\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16e4cf9c",
      "metadata": {},
      "source": [
        "# Modification of the beam due to Calsource uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6db311",
      "metadata": {},
      "outputs": [],
      "source": [
        "N = int(1e6)\n",
        "name_config = [\"Optimistic\", \"Realistic\", \"Pessimistic\"]\n",
        "labels = [r\"$\\delta_x$ (mm)\", r\"$\\delta_y$ (mm)\", r\"$\\delta_z$ (mm)\", r\"$\\delta_{el}$ ($1e^{-3}$ deg)\", r\"$\\delta_{az}$ ($1e^{-3}$ deg)\"]\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\"]\n",
        "sigma_colors = [\"#76b7b2\", \"#ffbb78\", \"#98df8a\", \"#ff9896\", \"#c5b0d5\"]\n",
        "config_indices = [12, 3, 37]\n",
        "\n",
        "corr = []\n",
        "\n",
        "for i in config_indices :\n",
        "    data = np.array([rpN_data[i], rpE_data[i], rpD_data[i], roll_data[i], yaw_data[i]])\n",
        "    corr.append(np.corrcoef(data, rowvar=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c899468",
      "metadata": {},
      "source": [
        "## Correlated Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0680e01e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_correlated_noise(corr_matrix, std_devs, n_samples):\n",
        "    C = np.diag(std_devs) @ corr_matrix @ np.diag(std_devs)\n",
        "\n",
        "    return np.random.multivariate_normal(\n",
        "        mean=np.zeros(C.shape[0]), \n",
        "        cov=C, \n",
        "        size=n_samples,\n",
        "    )\n",
        "\n",
        "# def generate_correlated_noise_from_ps(corr_matrix, std, n_samples, fs=sampling_rate):\n",
        "#     n_data = corr_matrix.shape[0]\n",
        "    \n",
        "#     # generate white noise with correlation\n",
        "#     white_noise = np.random.multivariate_normal(mean = np.zeros(n_data), cov=corr_matrix, size=n_samples).T\n",
        "    \n",
        "#     # \n",
        "#     correlated_noise = np.zeros((n_data, n_samples))\n",
        "#     freqs = np.fft.fftfreq(n_samples, 1/fs)\n",
        "#     df = fs / n_samples\n",
        "    \n",
        "#     for i,idata in enumerate(data_names):\n",
        "#         fit_param = dict_fit[\"values\"][idata]\n",
        "#         amp = np.sqrt(noise_model(np.abs(freqs), A_white=fit_param[\"A_white\"][config_index], f_knee=fit_param[\"f_knee\"][config_index], alpha=fit_param[\"alpha\"][config_index]) * df * n_samples)\n",
        "        \n",
        "#         amp[0] = amp[1] if amp[0] == np.inf else amp[0]\n",
        "        \n",
        "#         white_fft = np.fft.fft(white_noise[i])\n",
        "#         shaped_fft = white_fft * amp\n",
        "        \n",
        "#         correlated_noise[i] = np.real(np.fft.ifft(shaped_fft))\n",
        "    \n",
        "#     norm_factor = std / np.std(correlated_noise, axis=1)\n",
        "#     return correlated_noise * norm_factor[:, np.newaxis]\n",
        "\n",
        "def generate_correlated_noise_from_ps(R, n_samples, measured_stds, fs=sampling_rate, eps_reg=1e-14):\n",
        "    n_data = R.shape[0]\n",
        "    N = n_samples\n",
        "    freqs = np.fft.fftfreq(N, 1.0/fs)\n",
        "    df = fs / N\n",
        "\n",
        "    # --- 1) compute model PSD on grid for each channel ---\n",
        "    S_model = np.zeros((n_data, N), dtype=float)\n",
        "    for i, name in enumerate(data_names):\n",
        "        fp = dict_fit[\"values\"][name]\n",
        "        f_abs = np.maximum(np.abs(freqs), f_min)   # avoid f=0 singularity\n",
        "        S_vals = noise_model(f_abs,\n",
        "                             A_white=fp[\"A_white\"][config_index],\n",
        "                             f_knee=fp[\"f_knee\"][config_index],\n",
        "                             alpha=fp[\"alpha\"][config_index])\n",
        "        S_vals = np.asarray(S_vals, dtype=float)\n",
        "\n",
        "        # guard against NaN/inf\n",
        "        S_vals[~np.isfinite(S_vals)] = 0.0\n",
        "        S_model[i, :] = S_vals\n",
        "\n",
        "    # theoretical model variances (before scaling)\n",
        "    var_model = np.sum(S_model, axis=1) * df        # length n_data\n",
        "\n",
        "    # target variances from measured std\n",
        "    target_var = np.asarray(measured_stds, dtype=float)**2\n",
        "\n",
        "    # compute per-channel scale factors (guard divide-by-zero)\n",
        "    scale = np.ones(n_data)\n",
        "    mask_nonzero = var_model > 0\n",
        "    scale[mask_nonzero] = target_var[mask_nonzero] / var_model[mask_nonzero]\n",
        "    # if some var_model==0 (odd), keep scale=1 (or set to large number if you want)\n",
        "    # apply scale to PSD: S_scaled = scale[:,None] * S_model\n",
        "    S_scaled = S_model * scale[:, np.newaxis]\n",
        "\n",
        "    # --- generate multivariate colored noise using full cross-spectral factorization ---\n",
        "    # 1) independent white noise\n",
        "    white = np.random.randn(n_data, N)\n",
        "    W = np.fft.fft(white, axis=1)\n",
        "\n",
        "    # ensure R is PSD\n",
        "    R = (np.array(R, float) + np.array(R, float).T) / 2.0\n",
        "    wR, VR = np.linalg.eigh(R)\n",
        "    wR = np.clip(wR, 0.0, None)\n",
        "    R = (VR * wR[np.newaxis, :]) @ VR.T\n",
        "\n",
        "    X = np.zeros_like(W, dtype=np.complex128)\n",
        "\n",
        "    for k in range(N):\n",
        "        # S_vec for this freq using S_scaled\n",
        "        S_vec = S_scaled[:, k]\n",
        "        # sqrt diagonal\n",
        "        sqrtS = np.sqrt(np.maximum(S_vec, 0.0))\n",
        "        D = np.diag(sqrtS)\n",
        "        S_mat = D @ R @ D\n",
        "        S_mat = (S_mat + S_mat.T.conj()) / 2.0\n",
        "        S_mat += eps_reg * np.eye(n_data)\n",
        "\n",
        "        # eigen-decomp (small matrix)\n",
        "        w, V = np.linalg.eigh(S_mat)\n",
        "        w = np.clip(w, 0.0, None)\n",
        "        sqrt_diag = np.sqrt(w * df * N + 0.0)\n",
        "        M = (V * sqrt_diag[np.newaxis, :]) @ V.conj().T\n",
        "\n",
        "        X[:, k] = M @ W[:, k]\n",
        "\n",
        "    x = np.fft.ifft(X, axis=1).real\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d31904",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(config_indices), figsize=(15, 5))\n",
        "\n",
        "correlated_noise_white, correlated_noise_ps = [], []\n",
        "\n",
        "for i, config_index in enumerate(config_indices):\n",
        "    # Convert to mm and 1e-3 deg\n",
        "    correlated_noise_white.append(1000 * generate_correlated_noise(corr[i], std_data[i], N).T)\n",
        "    correlated_noise_ps.append(1000 * generate_correlated_noise_from_ps(corr[i], N, std_data[i]))\n",
        "\n",
        "    im = axes[i].imshow(corr[i], vmin=-1, vmax=1, cmap=\"bwr\", aspect='equal')\n",
        "    fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
        "\n",
        "    axes[i].set_xticks(np.arange(len(data_names)))\n",
        "    axes[i].set_yticks(np.arange(len(data_names)))\n",
        "    axes[i].set_xticklabels(data_names, rotation=45, ha='right')\n",
        "    axes[i].set_yticklabels(data_names)\n",
        "    axes[i].set_title(f\"Correlation matrix of the GPS data\\nConfig {name_config[i]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17091132",
      "metadata": {},
      "source": [
        "## Correlated Noise Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71392c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# White noise\n",
        "\n",
        "fig, axes = plt.subplots(1,len(config_indices), figsize=(15, 5))\n",
        "for i, config_index in enumerate(config_indices):\n",
        "    for j, data_name in enumerate(data_names):\n",
        "        axes[i].plot(correlated_noise_white[i][j], label=data_name + f\" {round(np.std(correlated_noise_white[i][j]), 4)}\", alpha=0.8, color=colors[j])\n",
        "    axes[i].set_title(f\"Config {name_config[i]}\")\n",
        "    axes[i].set_xlabel(\"Sample Index\")\n",
        "    axes[i].set_ylabel(\"Amplitude\")\n",
        "    axes[i].legend()\n",
        "    axes[i].set_ylim(-20, 20)\n",
        "fig.suptitle(\"Correlated Noise - White\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(len(config_indices), len(data_names), figsize=(22, 12), dpi=300)\n",
        "for iconfig in range(len(config_indices)):\n",
        "    for i in range(5):\n",
        "        ax = axes[iconfig, i]\n",
        "        data = correlated_noise_white[iconfig][i]\n",
        "        mean = np.mean(data)\n",
        "        std = np.std(data)\n",
        "        ax.hist(data, bins=50, color=colors[i], alpha=0.7, density=True)\n",
        "        ax.axvline(mean, linestyle=\"--\", color=\"k\", linewidth=2, label=f\"Mean = {mean:.4f}\")\n",
        "        ax.axvline(mean + std, linestyle=\"-\", color=sigma_colors[i], linewidth=2)\n",
        "        ax.axvline(mean - std, linestyle=\"-\", color=sigma_colors[i], linewidth=2)\n",
        "        ax.axvspan(mean - std, mean + std, color=sigma_colors[i], alpha=0.18, label=f\"±1σ = ±{std:.4f}\")\n",
        "        ax.set_xlabel(labels[i])\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        if iconfig == 0:\n",
        "            ax.set_title(labels[i])\n",
        "        if i == 0:\n",
        "            ax.annotate(name_config[iconfig], xy=(-0.2, 0.5), xycoords='axes fraction', fontsize=14,\n",
        "                        ha='right', va='center', rotation=90, fontweight='bold')\n",
        "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        ax.legend(fontsize=8, loc=\"upper right\")\n",
        "fig.suptitle(\"White Correlated Noise - Marginal Distributions\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c21b6b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# PS noise\n",
        "\n",
        "fig, axes = plt.subplots(1,len(config_indices), figsize=(15, 5))\n",
        "for i, config_index in enumerate(config_indices):\n",
        "    for j, data_name in enumerate(data_names):\n",
        "        axes[i].plot(correlated_noise_ps[i][j], label=data_name + f\" {round(np.std(correlated_noise_ps[i][j]), 4)}\", alpha=0.8, color=colors[j])\n",
        "    axes[i].set_title(f\"Config {name_config[i]}\")\n",
        "    axes[i].set_xlabel(\"Sample Index\")\n",
        "    axes[i].set_ylabel(\"Amplitude\")\n",
        "    axes[i].legend()\n",
        "    axes[i].set_ylim(-16.5, 16.5)\n",
        "fig.suptitle(\"Correlated Noise - Power Spectrum\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig, axes = plt.subplots(len(config_indices), len(data_names), figsize=(22, 12), dpi=300)\n",
        "for iconfig in range(len(config_indices)):\n",
        "    for i in range(5):\n",
        "        ax = axes[iconfig, i]\n",
        "        data = correlated_noise_ps[iconfig][i]\n",
        "        mean = np.mean(data)\n",
        "        std = np.std(data)\n",
        "        ax.hist(data, bins=50, color=colors[i], alpha=0.7, density=True)\n",
        "        ax.axvline(mean, linestyle=\"--\", color=\"k\", linewidth=2, label=f\"Mean = {mean:.4f}\")\n",
        "        ax.axvline(mean + std, linestyle=\"-\", color=sigma_colors[i], linewidth=2)\n",
        "        ax.axvline(mean - std, linestyle=\"-\", color=sigma_colors[i], linewidth=2)\n",
        "        ax.axvspan(mean - std, mean + std, color=sigma_colors[i], alpha=0.18, label=f\"±1σ = ±{std:.4f}\")\n",
        "        ax.set_xlabel(labels[i])\n",
        "        ax.set_ylabel(\"Density\")\n",
        "        if iconfig == 0:\n",
        "            ax.set_title(labels[i])\n",
        "        if i == 0:\n",
        "            ax.annotate(name_config[iconfig], xy=(-0.2, 0.5), xycoords='axes fraction', fontsize=14,\n",
        "                        ha='right', va='center', rotation=90, fontweight='bold')\n",
        "        ax.grid(True, linestyle=\"--\", alpha=0.5)\n",
        "        ax.legend(fontsize=8, loc=\"upper right\")\n",
        "fig.suptitle(\"Power Spectrum Correlated Noise - Marginal Distributions\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75d55260",
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(config_indices), figsize=(20, 5))\n",
        "\n",
        "fig_, axes_ = plt.subplots(1, len(name_config), figsize=(15, 5))\n",
        "\n",
        "for i, config in enumerate(name_config):\n",
        "    corr_test = np.corrcoef(correlated_noise_ps[i], rowvar=True)\n",
        "    im = axes[i].imshow(corr_test, vmin=-1, vmax=1, cmap=\"bwr\", aspect='equal')\n",
        "    fig.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n",
        "\n",
        "    axes[i].set_xticks(np.arange(len(data_names)))\n",
        "    axes[i].set_yticks(np.arange(len(data_names)))\n",
        "    axes[i].set_xticklabels(data_names, rotation=45, ha='right')\n",
        "    axes[i].set_yticklabels(data_names)\n",
        "    axes[i].set_title(f\"Correlation matrix of the GPS data\\nConfig {config}\")\n",
        "    \n",
        "    im = axes_[i].imshow(corr_test - corr[i], vmin=-1, vmax=1, cmap=\"bwr\", aspect='equal')\n",
        "    fig_.colorbar(im, ax=axes_[i], fraction=0.046, pad=0.04)\n",
        "\n",
        "    axes_[i].set_xticks(np.arange(len(data_names)))\n",
        "    axes_[i].set_yticks(np.arange(len(data_names)))\n",
        "    axes_[i].set_xticklabels(data_names, rotation=45, ha='right')\n",
        "    axes_[i].set_yticklabels(data_names)\n",
        "    axes_[i].set_title(f\"Config {config}\")\n",
        "fig_.suptitle(\"Diff Correlation matrix between measured and simulated data\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6727003",
      "metadata": {},
      "source": [
        "# MC Gaussian Noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef8b970",
      "metadata": {},
      "source": [
        "### 1. **Physical Context and Coordinate System**\n",
        "\n",
        "- The calibration source emits a beam toward the QUBIC receiver.\n",
        "- The **x-axis** is defined along the line joining the source and QUBIC (beam axis).\n",
        "- The **z-axis** is vertical (upwards).\n",
        "- The **y-axis** is horizontal and orthogonal to both x and z.\n",
        "\n",
        "In this frame:\n",
        "- **Azimuth (`az`)** is a rotation around the z-axis (vertical), causing the beam to sweep left-right (modifies y).\n",
        "- **Elevation (`el`)** is a rotation around the y-axis (horizontal), causing the beam to sweep up-down (modifies z).\n",
        "\n",
        "### 2. **Uncertainties**\n",
        "\n",
        "- The calibration source position is uncertain in all three spatial directions: `d_x`, `d_y`, `d_z`.\n",
        "- The orientation is also uncertain: `d_el` (elevation), `d_az` (azimuth).\n",
        "- These uncertainties are sampled from a multivariate normal distribution.\n",
        "\n",
        "### 3. **Propagation of Uncertainties**\n",
        "\n",
        "#### **a. QUBIC Position Vector**\n",
        "\n",
        "We define the QUBIC position in the source frame as:\n",
        "```python\n",
        "qubic_vec = np.array([distance_qubic_calsource, 0, 0])\n",
        "```\n",
        "This means QUBIC is located at a distance `distance_qubic_calsource` along the x-axis from the source.\n",
        "\n",
        "#### **b. Applying Rotations**\n",
        "\n",
        "To account for orientation uncertainties, we rotate the QUBIC vector by the sampled elevation and azimuth angles:\n",
        "\n",
        "- **Elevation rotation (`R_el`)**: Rotates around the y-axis.\n",
        "- **Azimuth rotation (`R_az`)**: Rotates around the z-axis.\n",
        "\n",
        "The combined rotation is:\n",
        "```python\n",
        "R = R_az @ R_el\n",
        "qubic_rot = R @ qubic_vec\n",
        "```\n",
        "This gives the new direction of the beam after applying the sampled orientation errors.\n",
        "\n",
        "#### **c. Adding Position Uncertainties**\n",
        "\n",
        "After rotation, we add the sampled position uncertainties:\n",
        "```python\n",
        "qubic_rot += np.array([d_x, d_y, d_z])\n",
        "```\n",
        "This shifts the QUBIC position according to the sampled errors.\n",
        "\n",
        "#### **d. Calculating the Offset from the Beam Axis**\n",
        "\n",
        "The **radial offset** from the beam axis (which is now possibly misaligned) is:\n",
        "```python\n",
        "delta_r = np.sqrt(qubic_rot[1]**2 + qubic_rot[2]**2)\n",
        "```\n",
        "This is the distance from the beam axis in the y-z plane.\n",
        "\n",
        "The **longitudinal offset** (distance along the beam axis) is:\n",
        "```python\n",
        "delta_x = qubic_rot[0]\n",
        "```\n",
        "This is the new distance from the source along the (possibly rotated) x-axis.\n",
        "\n",
        "#### **e. Calculating the Received Intensity**\n",
        "\n",
        "The received intensity at QUBIC is then computed using the Gaussian beam formula:\n",
        "```python\n",
        "amplitudes[i] = gaussian_beam(delta_r, delta_x, w0, I0) / intensity_normalization\n",
        "```\n",
        "- `gaussian_beam(r, z, w0, I0)` gives the intensity at radial offset `r` and distance `z` from the source.\n",
        "- We normalize by the intensity at the nominal position to get a relative amplitude."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f99814e",
      "metadata": {},
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d2e945",
      "metadata": {},
      "outputs": [],
      "source": [
        "amplitudes_white = []\n",
        "amplitudes_ps = []\n",
        "labels_conf = []\n",
        "\n",
        "def uncertainties_from_noise(noise): \n",
        "    d_x, d_y, d_z, d_el, d_az = noise  / 1000 # / 1000 because my noise are in mm and 1e-3 rad\n",
        "    el_rad, az_rad = np.radians(d_el), np.radians(d_az)\n",
        "    N_noise = len(d_x)\n",
        "    \n",
        "    cos_el = np.cos(el_rad)\n",
        "    sin_el = np.sin(el_rad)\n",
        "    cos_az = np.cos(az_rad)\n",
        "    sin_az = np.sin(az_rad)\n",
        "\n",
        "    qubic_vec = np.array([distance_qubic_calsource, 0, 0])\n",
        "\n",
        "    x1 = qubic_vec[0] * cos_el + qubic_vec[2] * sin_el\n",
        "    y1 = qubic_vec[1] * np.ones(N_noise)\n",
        "    z1 = -qubic_vec[0] * sin_el + qubic_vec[2] * cos_el\n",
        "\n",
        "    x2 = x1 * cos_az - y1 * sin_az\n",
        "    y2 = x1 * sin_az + y1 * cos_az\n",
        "    z2 = z1\n",
        "\n",
        "    x_final = x2 + d_x\n",
        "    y_final = y2 + d_y\n",
        "    z_final = z2 + d_z\n",
        "\n",
        "    delta_r = np.sqrt(y_final**2 + z_final**2)\n",
        "    delta_x = x_final\n",
        "    \n",
        "    return gaussian_beam(delta_r, delta_x, w0, I0) / gaussian_beam(0, distance_qubic_calsource, w0, I0)\n",
        "\n",
        "for iconfig in range(len(config_indices)):\n",
        "    amplitudes_white.append(uncertainties_from_noise(correlated_noise_white[iconfig]))\n",
        "    amplitudes_ps.append(uncertainties_from_noise(correlated_noise_ps[iconfig]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03271d50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot amplitude distributions for the 3 configurations\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors_ = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "for i, (amplitudes, label) in enumerate(zip(amplitudes_white, name_config)):\n",
        "    plt.hist(amplitudes, bins=50, density=True, alpha=0.5, color=colors[i], label=name_config[i])\n",
        "    mean = np.mean(amplitudes)\n",
        "    std = np.std(amplitudes)\n",
        "    plt.axvline(mean, color=colors[i], linestyle='--', linewidth=2, label=f'{name_config[i]} mean = {mean:.4f}')\n",
        "    plt.axvspan(mean-std, mean+std, color=colors[i], alpha=0.12, label=f'{name_config[i]} ±1σ = ±{std:.4e}')\n",
        "\n",
        "plt.xlabel('Amplitude (normalized)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Monte Carlo Amplitude Distributions for 3 Configurations - White Noise')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot amplitude distributions for the 3 configurations\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors_ = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
        "for i, (amplitudes, label) in enumerate(zip(amplitudes_ps, name_config)):\n",
        "    plt.hist(amplitudes, bins=50, density=True, alpha=0.5, color=colors[i], label=name_config[i])\n",
        "    mean = np.mean(amplitudes)\n",
        "    std = np.std(amplitudes)\n",
        "    plt.axvline(mean, color=colors[i], linestyle='--', linewidth=2, label=f'{name_config[i]} mean = {mean:.4f}')\n",
        "    plt.axvspan(mean-std, mean+std, color=colors[i], alpha=0.12, label=f'{name_config[i]} ±1σ = ±{std:.4e}')\n",
        "\n",
        "plt.xlabel('Amplitude (normalized)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Monte Carlo Amplitude Distributions for 3 Configurations - PS Noise')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a18b48f",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1fa230b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbe1e759",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3e8ba4f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}
